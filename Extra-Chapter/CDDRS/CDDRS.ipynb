{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1f9d85",
   "metadata": {},
   "source": [
    "# 建筑文档智能RAG审查系统\n",
    "\n",
    "一个从零开始实现的建筑文档智能审查系统，旨在帮助开发者理解知识引导检索在专业领域文档审查中的核心原理和实现细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248f779",
   "metadata": {},
   "source": [
    "## 项目动机\n",
    "\n",
    "建筑施工交底文档的合规性审查是保障施工项目安全性、经济性的关键环节。在施工项目全周期中，各项操作必须符合相关规范条文要求，才能确保建设项目的安全性与可持续性。然而，相关查询参考往往分散在各个项目文件中，传统基于人工的审查方法难以处理庞大复杂的建筑条文，其审查过程需要基于审查人员的经验与专业知识，具有主观性强，耗时长且易出错等弊端。\n",
    "\n",
    "随着大语言模型技术的发展，LLM为自动化建筑文档审查带来了新的希望。然而，大语言模型通常使用通用语料进行训练，缺乏建筑相关背景知识，在处理建造背景下的复杂推理问题中会产生严重的幻觉现象。通过使用基于向量相似匹配的RAG方法，可以为LLMs提供初步的相似参考知识，从而减轻基于人工或规则的审查方法难以处理庞大建筑文本所带来的错误率高的问题。\n",
    "\n",
    "然而，传统RAG方法在建筑专业文档审查中存在关键局限：由于固定的分块设计，使得文本块之间面临知识信息缺失问题；在检索过程中，使用整句问询嵌入的方法进行相似性匹配，缺少对问询细粒度特征的识别与考量，检索效率低下。在建筑施工交底文档中，这类文档详细阐述了施工工艺特点和方法、质量规格、操作程序以及安全协议，包含大量知识细节且专业性极强。因此需要一个能够精准理解和检索建筑领域专业知识的智能系统。\n",
    "\n",
    "因此，本项目提出了一个生成式知识引导的建筑文档审查系统，旨在提升审查的可靠性和准确性。系统具有两大核心创新：首先提出动态语义知识分块策略，构建具有更优语义连贯性和完整性的知识库；其次基于增强的知识表示，提出生成式知识引导检索框架，在语义嵌入检索过程中增强对细粒度信息的关注，从而提高知识参考检索的准确性和建筑文档审查任务中修正的可靠性。\n",
    "\n",
    "需要注意的是，由于篇幅限制，我们无法展示完整的整个实现过程，但是，我们将在文档中讲解每个必要的实现步骤以及背后的思考，您可以通过这些内容快速理解如何实现一个建筑文档智能审查系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f052666",
   "metadata": {},
   "source": [
    "## 前置实现\n",
    "\n",
    "接下来，我们将带领大家，从0开始，实现一个建筑文档智能审查系统。首先，我们将完成一些基本的准备过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df1ef0",
   "metadata": {},
   "source": [
    "### 1. 实现 LLM 模块\n",
    "\n",
    "首先我们需要实现 LLM 模块，这是系统中最基本的模块，我们将利用大模型完成文档的清洗，信息提取等工作，可以说本系统的一部分精髓即为使用大模型预先处理文档信息，方便后续进行检索，这里我们使用 DeepSeek 的 api 来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e0d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Optional\n",
    "\n",
    "class BaseLLM(ABC):\n",
    "    \"\"\"Interface for large language models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        model_params: Optional[dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model_params = model_params or {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, input: str) -> str:\n",
    "        \"\"\"Sends a text input to the LLM and retrieves a response.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0783425",
   "metadata": {},
   "source": [
    "如上是一个调用大模型的抽象接口，这可以帮助我们统一调用大模型的格式，我们继承这个基类，实现调用大模型的接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80f5410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Any, Optional\n",
    "\n",
    "class DeepSeekLLM(BaseLLM):\n",
    "    \"\"\"Implementation of the BaseLLM interface using DeepSeek API.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        api_key: str,\n",
    "        base_url: str = \"https://api.deepseek.com/v1\",\n",
    "        model_params: Optional[dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        super().__init__(model_name, model_params, **kwargs)\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "    def predict(self, input: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": input}],\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e82db8",
   "metadata": {},
   "source": [
    "完成搭建后，我们可以通过尝试调用 predict 方法来测试是否成功。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910377cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以！我很乐意帮助您进行建筑文档审查。不过，为了提供更准确的建议，我需要了解一些具体信息：\n",
      "\n",
      "**请提供以下细节（根据实际情况选择分享）：**\n",
      "1. **文档类型**：是施工图、技术规范、合同文件、设计说明、工程量清单（BOQ）还是其他？\n",
      "2. **具体需求**：您希望审查哪些方面？（例如：规范性、合规性、技术细节、数据一致性、错漏检查等）\n",
      "3. **项目类型**：住宅、商业建筑、工业设施还是基础设施？\n",
      "4. **关注重点**：是否有特定标准需要符合（如国家标准、绿色建筑认证、安全规范等）？\n",
      "\n",
      "---\n",
      "\n",
      "### 我能协助的常见审查方向：\n",
      "1. **格式与完整性**  \n",
      "   - 检查目录结构、编号系统、图表标注是否清晰一致。  \n",
      "   - 确认关键章节（如设计说明、材料规格、施工要求）是否齐全。\n",
      "\n",
      "2. **合规性提示**（需提供当地规范名称）  \n",
      "   - 防火疏散、无障碍设计、结构荷载等基本规范（例如：中国《建筑设计防火规范》GB 50016）。  \n",
      "   - 环保或节能要求（如LEED、BREEAM或中国绿色建筑标准）。\n",
      "\n",
      "3. **逻辑一致性**  \n",
      "   - 平面图、立面图、剖面图之间的尺寸与标注是否冲突。  \n",
      "   - 设计说明与技术细节是否匹配（如材料规格与施工工艺）。\n",
      "\n",
      "4. **常见错误排查**  \n",
      "   - 单位错误（如mm与cm混淆）、标注遗漏、图例错误等。  \n",
      "   - 工程量清单（BOQ）中项目与图纸数量是否一致。\n",
      "\n",
      "5. **术语与表述优化**  \n",
      "   - 建议更专业的表述方式或术语标准化。\n",
      "\n",
      "---\n",
      "\n",
      "### 注意事项：\n",
      "- 我是AI，无法替代专业工程师或律师的审核，但可提供初步参考意见。  \n",
      "- 涉及结构安全、法规强制要求的内容，请务必由持证专业人员复核。  \n",
      "- 您可以直接发送具体段落或描述问题，我会尽力协助！\n",
      "\n",
      "请提供更多细节，我会为您提供针对性建议。\n"
     ]
    }
   ],
   "source": [
    "llm = DeepSeekLLM(\n",
    "    model_name=\"deepseek-chat\", \n",
    "    api_key=\"your-api-key\",\n",
    "    base_url=\"https://api.deepseek.com/v1\"\n",
    ")\n",
    "print(llm.predict(\"你好，你能帮助我进行建筑文档审查吗？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8506fa9",
   "metadata": {},
   "source": [
    "当观察到 LLM 正确回复后，我们这一模块的构建就完成了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d049de7",
   "metadata": {},
   "source": [
    "### 2. 实现 Embedding 模块\n",
    "\n",
    "除了调用大模型，我们还需要实现 Embedding 模块，Embedding 模块用于将文本转换为向量，我们将使用向量来表示文档中的信息，这样的好处是，我们可以通过向量的相似度来衡量文档与查询之间的相似度，从而召回对回复用户问题最有帮助的文档。\n",
    "\n",
    "构建 Embedding 模块的方法与构建 LLM 模块类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f7b8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llamaindex\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "class BaseEmb(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        model_params: Optional[dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model_params = model_params or {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_emb(self, input: str) -> List[float]:\n",
    "        \"\"\"Sends a text input to the embedding model and retrieves the embedding.\"\"\"\n",
    "        pass\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "class BGEEmbedding(BaseEmb):\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-m3\", **kwargs):\n",
    "        super().__init__(model_name=model_name, **kwargs)\n",
    "        self.embed_model = HuggingFaceEmbedding(\n",
    "            model_name=model_name,\n",
    "            trust_remote_code=True,\n",
    "            cache_folder=\"./model_cache\"\n",
    "        )\n",
    "\n",
    "    def get_emb(self, text: str) -> List[float]:\n",
    "        embedding = self.embed_model.get_text_embedding(text)\n",
    "        return embedding\n",
    "    \n",
    "    def encode(self, texts, show_progress_bar=False):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            emb = self.get_emb(text)\n",
    "            embeddings.append(emb)\n",
    "        \n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3c300",
   "metadata": {},
   "source": [
    "完成搭建后，我们可以通过尝试调用 get_emb 方法来测试是否成功。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf21d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.008159242570400238, 0.005754395853728056, -0.024663547053933144, 0.0038987270090729, 0.0221844669431448, 0.0007059215568006039, 0.004481502342969179, -0.0015061416197568178, -0.022438108921051025, -0.0030729181598871946, -0.028854981064796448, 0.0223580002784729, -0.014190766029059887, -0.02054421603679657, -0.0019247722811996937, -0.041896067559719086, 0.013186885975301266, -0.04618095979094505, 0.009962386451661587, -0.002442343160510063, -0.013730430044233799, -0.0670018196105957, -0.028913620859384537, -0.01982208900153637, 0.02447838895022869, 0.03258971497416496, -0.03164232149720192, -0.002734268084168434, -0.020909657701849937, 0.05742897093296051, 0.021285204216837883, -0.02225871942937374, 0.07097937166690826, -0.004255959764122963, 0.021880434826016426, -0.04358939453959465, 0.047468990087509155, -0.0665409043431282, -0.05259682610630989, 0.004260585643351078, -0.010267225094139576, -0.03244507685303688, 0.0223003588616848, -0.019258055835962296, -0.0003713485202752054, -0.04361735284328461, 0.026206018403172493, -0.03023737110197544, 0.013313274830579758, 0.0002251705009257421, 0.008343675173819065, -0.030304472893476486, -0.004975242540240288, -0.026585601270198822, 0.011065107770264149, 0.017322678118944168, 0.02283507212996483, -0.0435573048889637, -0.05924893915653229, -0.022334391251206398, -0.030627364292740822, 0.029643332585692406, -0.004384420812129974, -0.001198050333186984, 0.03390760347247124, 0.01960756443440914, -0.005116869695484638, -0.04911072552204132, -0.007285997737199068, -0.06285838782787323, 0.02450537495315075, -0.02004382573068142, -0.010927724651992321, 0.028427692130208015, -0.04490334168076515, -0.011161834001541138, 0.005235197953879833, -0.01957201212644577, -0.04021269828081131, -0.02049367129802704, -0.03427192196249962, 0.050749681890010834, 0.026393454521894455, 0.02682250551879406, -0.008772539906203747, 0.025520754978060722, -0.010053625330328941, 0.03046583943068981, 0.015397612005472183, -0.0509209930896759, -0.0539475716650486, 0.004840736277401447, 0.03514740616083145, -0.0034363295417279005, -0.055750150233507156, -0.0026254449039697647, -0.004683077801018953, -0.03790610656142235, 0.03699930012226105, -0.0014170100912451744, 0.007965784519910812, -0.051486629992723465, 0.028585148975253105, -0.019408004358410835, 0.03357052803039551, -0.02611122652888298, 0.016895202919840813, 0.005833339411765337, -0.005644122138619423, -0.013383697718381882, 0.015408149920403957, -0.011529901996254921, 0.033537622541189194, 0.006104541476815939, 0.0181846022605896, -0.025680096819996834, 0.009130419231951237, -0.022236522287130356, -0.03444993868470192, 0.002107771812006831, 0.06445536017417908, -0.015476440079510212, 0.04229044169187546, -0.061122022569179535, 0.014955008402466774, -0.0019445978105068207, -0.003454635851085186, 0.04888900741934776, 0.0026477354113012552, 0.03745702654123306, 0.002253544284030795, 0.016443034633994102, -0.00763604324311018, -0.016121556982398033, -0.028234101831912994, 0.02534499578177929, -0.002640634309500456, 0.04827441647648811, 0.016369467601180077, -0.026182588189840317, 0.05507376044988632, 0.008636887185275555, -0.02046351693570614, -0.019356289878487587, 0.01482913177460432, -0.019503336399793625, 0.004169851075857878, -0.026032783091068268, 0.030682329088449478, -0.04218831658363342, 0.0005025395657867193, 0.0010827184887602925, 0.04963816702365875, 0.02726980298757553, 0.02827153541147709, 0.0007039656047709286, -0.039251312613487244, -0.009724192321300507, 0.04078090563416481, -0.016981666907668114, 0.013814247213304043, 0.0032257679849863052, -0.0013130503939464688, 0.014551229774951935, 0.010753488168120384, -0.00707739032804966, -0.0038439284544438124, -0.005789074581116438, -0.04882612079381943, 0.033391907811164856, -0.016739539802074432, -0.05458156019449234, -0.005394328851252794, -0.015763062983751297, 0.013416395522654057, -0.0021539125591516495, 0.08293414115905762, 0.030252447351813316, -0.017012350261211395, -0.042549606412649155, -0.01920093223452568, 0.025530759245157242, -0.02751781791448593, 0.032338351011276245, -0.0034977809991687536, 0.010373717173933983, 0.024666251614689827, 0.02892077900469303, 0.015242754481732845, 0.014347690157592297, -0.01561523787677288, -0.02215687930583954, 0.06355950981378555, -0.03061477281153202, 0.013036615215241909, 0.007974004372954369, 0.04307020828127861, 0.04244697839021683, 0.055088672786951065, 0.010855956934392452, 0.020225398242473602, -0.0047472394071519375, 0.049362748861312866, -0.014738965779542923, -0.0182197168469429, 0.0019958424381911755, -0.0336240753531456, -0.030366841703653336, 0.019157813861966133, -0.04555022343993187, -0.01577058620750904, -0.01285738218575716, 0.048405565321445465, 0.03940584138035774, -0.03844134137034416, 0.023708857595920563, -0.025616571307182312, 0.0037186769768595695, -0.014304162934422493, 0.00843740813434124, -0.012598640285432339, -0.01618318073451519, 0.010255432687699795, -0.05655519291758537, 0.034144721925258636, 0.0007591137546114624, 0.016931485384702682, 0.011007283814251423, 0.02576807513833046, -0.04384667053818703, 0.01992262899875641, 0.01989668421447277, -0.00542093813419342, -0.03282667696475983, 0.05913662537932396, 0.04952588677406311, 0.009882078506052494, 0.018577586859464645, 0.05728962644934654, -0.010044380091130733, 0.012747026048600674, -0.06368867307901382, 0.044708989560604095, -0.010439618490636349, -0.025499198585748672, -0.02203153818845749, 0.01491384394466877, 0.02911507338285446, -0.03340592607855797, -0.027025623247027397, 0.0016051143174991012, -0.024251878261566162, 0.014879211783409119, -0.004962841048836708, 0.03983527794480324, 0.01239701546728611, -0.00918954610824585, -0.011987953446805477, 0.048828013241291046, 0.0033441183622926474, 0.033875368535518646, 0.038586921989917755, 0.00328968558460474, 0.025484472513198853, 0.04570803418755531, -0.007011420093476772, -0.016749851405620575, -0.02737160213291645, 0.0074218763038516045, 0.013574519194662571, 0.008099310100078583, 0.019064737483859062, 0.02236461639404297, -0.021313967183232307, -0.010747293941676617, 0.03744882345199585, -0.0050834789872169495, 0.003102300688624382, 0.06860381364822388, -0.00735899293795228, -0.0007147222640924156, -0.05509135127067566, -0.011860791593790054, 0.00011540263221831992, -0.0013021506601944566, -0.05093668773770332, -0.020819270983338356, -0.04814756289124489, -0.002467456506565213, -0.05456923320889473, 0.0015354141360148787, -0.048741698265075684, 0.08611465245485306, -0.030389651656150818, 0.020445002242922783, 0.015324302949011326, -0.005680671893060207, -0.15097397565841675, -0.0018968378426507115, -0.01568077690899372, 0.025341948494315147, 0.0059852986596524715, -0.0011712362756952643, -0.023526951670646667, -0.03041907772421837, -0.04523899406194687, 0.00013821253378409892, -0.06493302434682846, -0.06562570482492447, 0.009819632396101952, -0.00178980128839612, 0.05128961801528931, -0.0014715674333274364, -0.04730747640132904, 0.021933168172836304, -0.010594665072858334, -0.028410566970705986, -0.018882660195231438, -0.00906360987573862, 0.06769031286239624, 0.02508336678147316, 0.03553688898682594, -0.020766498520970345, 0.07419787347316742, -0.02149971015751362, -0.024415716528892517, 0.013685734942555428, -0.016621537506580353, 0.008740579709410667, -0.008354954421520233, 0.007627969607710838, 0.04790123179554939, -0.0035329859238117933, 0.008899709209799767, -0.02104680798947811, -0.025366663932800293, 0.029143797233700752, 0.019871309399604797, 0.015991905704140663, -0.01845811866223812, -0.0166324395686388, 0.024127844721078873, -0.05731573700904846, 0.028131384402513504, -0.01046037208288908, 0.001185560249723494, -0.0060633160173892975, -0.044790688902139664, 0.010853624902665615, -0.004852153826504946, -0.00883186049759388, -0.06479669362306595, -0.013680394738912582, -0.03005005232989788, 0.01836826652288437, -0.02213100716471672, -0.014201287180185318, 0.015034623444080353, -0.08682654798030853, -0.005147515796124935, -0.02640114352107048, 0.018159715458750725, -0.0361277274787426, -0.012911590747535229, -0.00467587960883975, -0.009012139402329922, -0.010229379869997501, -0.013843361288309097, -0.03788858652114868, -0.042526692152023315, -0.04254135489463806, 0.01150231808423996, 0.02659926377236843, -0.018250424414873123, 0.04497944936156273, -0.00014919576642569155, -0.09051945805549622, 0.011776473373174667, 0.007612489629536867, 0.0015711288433521986, 0.004494070075452328, -0.03757219389081001, 0.007738599553704262, 0.011990596540272236, 0.028499284759163857, 0.022048959508538246, 0.1985333412885666, -0.04426541179418564, -0.037347856909036636, -0.027941251173615456, 0.0010355062549933791, -0.019851993769407272, 0.025895407423377037, 0.020690426230430603, 0.0013027037493884563, -0.03536158800125122, -0.06401663273572922, 0.02169177308678627, 0.027167029678821564, -0.00950489193201065, 0.0383734405040741, -0.026381129398941994, -0.016536174342036247, 0.025340868160128593, 0.09294909983873367, -0.013789238408207893, 0.01720455102622509, 0.012751719914376736, 0.0008590622455812991, 0.007423927541822195, -0.05435327813029289, -0.03942925110459328, -0.0157479178160429, 0.04002939537167549, -0.037848036736249924, 0.005167141556739807, -0.004750934895128012, 0.027016988024115562, 0.03793930634856224, -0.0021914951503276825, -0.03640374541282654, 0.043172162026166916, -0.017139365896582603, 0.02823808044195175, -0.05737394094467163, 0.048020925372838974, 0.004737760405987501, -0.01879766955971718, -0.02250038832426071, -0.0013258701656013727, -0.017133887857198715, -0.024853866547346115, -0.005083943717181683, 0.013188464567065239, -0.016682926565408707, 0.018507882952690125, -0.014458547346293926, 0.025885671377182007, -0.02617962658405304, 0.026932578533887863, -0.034904152154922485, 0.042742617428302765, -0.029168149456381798, -0.026066835969686508, -0.00531662767753005, 0.046957630664110184, 0.033596962690353394, -0.01823245733976364, -0.035914432257413864, 0.012456627562642097, 0.02694978378713131, -0.0066740200854837894, -0.03440346196293831, -0.013075420632958412, 0.031181251630187035, 0.035029660910367966, 0.06028195098042488, -0.023016994819045067, -0.019278833642601967, 0.004756495822221041, 0.03375618904829025, 0.008311769925057888, 0.030073100700974464, 0.0760876014828682, -0.00802715215831995, 0.02579597197473049, 0.0006852707592770457, 0.014750344678759575, 0.0017241452587768435, 0.005290407687425613, 0.011468152515590191, 0.027898697182536125, -0.06464944034814835, 0.004964817781001329, 0.02021591179072857, 0.012583546340465546, -0.01791338622570038, -0.0024353712797164917, -0.022329390048980713, 0.0021304956171661615, 0.0036976851988583803, 0.03899001330137253, 0.011300807818770409, -0.014965500682592392, -0.046065475791692734, -0.03753596916794777, 0.027129262685775757, -0.024559300392866135, -0.02639039047062397, 0.022491294890642166, 0.009033432230353355, -0.008362609893083572, 0.0001487734989495948, 0.039778005331754684, 0.014077797532081604, 0.05170590430498123, -0.022254720330238342, 0.016157470643520355, 0.002221450675278902, -0.014121792279183865, -0.029643535614013672, 0.002873690566048026, -0.037266165018081665, 0.014971030876040459, 0.061250921338796616, 0.04801003262400627, -0.010305630974471569, 0.07863421738147736, -0.028787901625037193, -0.0012510212836787105, -0.05323777347803116, -0.0009261402883566916, -0.05675649270415306, -0.03329698368906975, -0.0021774962078779936, 0.06076507270336151, 0.02608843520283699, -0.013583504594862461, -0.02154897153377533, -0.013955802656710148, 0.05170121416449547, 0.007481550797820091, -0.004648298025131226, 0.051262687891721725, 0.000945074949413538, 0.022759323939681053, -0.010619102045893669, -0.012822180055081844, -0.030171940103173256, -0.01176955085247755, -0.016073077917099, -0.01429969072341919, -0.01441648043692112, -0.01920558325946331, -0.002067919122055173, -0.062922403216362, -0.007660774048417807, 0.00793564971536398, -0.007822266779839993, -0.06949237734079361, 0.04801243171095848, 0.0036780438385903835, -0.023699555546045303, -0.007212137337774038, -2.1765999917988665e-05, -0.0345299057662487, 0.02497580088675022, -0.014896790497004986, -0.008217303082346916, 0.07821899652481079, 0.047570597380399704, 0.015609372407197952, 0.004776008892804384, 0.012515947222709656, 0.011383699253201485, 0.005764748901128769, 0.05192238837480545, -0.007776946295052767, -0.06453675776720047, 0.04706624895334244, 0.08395755290985107, -0.01908132992684841, -0.11242877691984177, -0.011603251099586487, 0.03921223804354668, 0.027820877730846405, 0.042384177446365356, -0.0036653948482125998, -0.01978554204106331, -0.022139208391308784, -0.014130666851997375, -0.011148289777338505, 0.024651428684592247, -0.021974893286824226, 0.04416286572813988, -0.025216246023774147, -0.02720513381063938, 0.08111189305782318, -0.00949477031826973, -0.005520079284906387, 0.048297442495822906, -0.011168230324983597, -0.02132153883576393, -0.013992955908179283, -0.026559675112366676, 0.026964262127876282, -0.0038850067649036646, 0.0009995036525651813, 0.014295326545834541, 0.024056699126958847, -0.021158399060368538, -0.02968648448586464, -0.046675484627485275, 0.010086658410727978, -0.04817875474691391, 0.0017965140286833048, 0.010042015463113785, 0.012909766286611557, 0.010214140638709068, 0.008607500232756138, -0.0408991314470768, 0.014573400840163231, -0.025889338925480843, 0.04696730896830559, 0.0545382983982563, -0.04756659269332886, 0.004670929163694382, 0.025322075933218002, 0.04067161679267883, -0.030523329973220825, 0.023255759850144386, 0.008356871083378792, 0.05639335513114929, -0.02657676860690117, 0.01303358469158411, -0.001974142389371991, 0.005305653903633356, -0.01812334917485714, -0.014432300813496113, -0.04239063709974289, 0.035735342651605606, -0.0253747571259737, -0.04058588296175003, 0.02939145639538765, -0.014323804527521133, -0.014731739647686481, -0.00027179718017578125, -0.004914026241749525, 0.04327885061502457, 0.013477833941578865, 0.023266315460205078, 0.024892501533031464, -0.020088836550712585, 0.017984040081501007, -0.004191387444734573, 0.032651517540216446, 0.009538357146084309, -0.0042325942777097225, 0.022759458050131798, -0.0028504966758191586, -0.031356580555438995, -0.0017651666421443224, -0.0017029891023412347, -0.011442538350820541, -0.010529697872698307, 0.000798829656559974, -0.02697906456887722, -0.012902587652206421, -0.0326249897480011, 0.020969677716493607, -0.002441784832626581, -0.049833089113235474, 0.017128603532910347, -0.01148754172027111, -0.04498909413814545, 0.009785826317965984, -0.026679018512368202, -0.011309775523841381, -0.03883408382534981, -0.0032419587951153517, -0.07190737873315811, -0.0025989296846091747, -0.07983973622322083, 0.027425453066825867, -0.039434172213077545, 0.000113294692710042, 0.043311141431331635, 0.008025635965168476, -0.009404486045241356, 0.03524414449930191, -0.0018946131458505988, -0.027102893218398094, -0.005990078207105398, -0.01817081868648529, -0.007102642208337784, 0.004550714511424303, 0.03653610870242119, 0.011947095394134521, -0.022481150925159454, -0.02041434496641159, 0.001818244461901486, 0.06134777143597603, -0.035601906478405, 0.035721637308597565, -0.045185357332229614, 0.02865186519920826, -0.019968224689364433, 0.012533607892692089, -0.05320208519697189, 0.0040062167681753635, 0.0030551603995263577, 0.027050429955124855, 0.005466382950544357, 0.03785153105854988, 0.016277272254228592, -0.008986697532236576, 0.012693117372691631, -0.08303315937519073, 0.036484960466623306, -0.028784997761249542, 0.006555391009896994, -0.01160408928990364, 0.016153018921613693, -0.040073759853839874, 0.010013018734753132, 0.03954056277871132, -0.011690113693475723, 0.01969950459897518, -0.013315856456756592, -0.016824085265398026, 0.0038188984617590904, -0.006109373643994331, -0.017293602228164673, 0.04615958407521248, -0.00044023891678079963, 0.007981272414326668, -0.05818289890885353, -0.0026455027982592583, -0.01792796142399311, 0.0014871961902827024, 0.03274844214320183, 0.04940430447459221, 0.0017527458257973194, -0.03890381380915642, 0.05372945964336395, 0.003851843299344182, 0.02785513736307621, -0.022414304316043854, 0.0451260544359684, -0.008142136037349701, -0.008295497857034206, -0.058595504611730576, 0.0007978644571267068, -0.030595725402235985, -0.022097865119576454, 0.014339243061840534, -0.00782725028693676, 0.0245810579508543, -0.0063614631071686745, -0.055110398679971695, -0.010170851834118366, 0.03121359832584858, -0.004042973276227713, 0.00584474578499794, 0.026070119813084602, -0.009010711684823036, 0.03935391083359718, 0.00025671112234704196, 0.014945556409657001, -0.029459843412041664, -0.023990990594029427, -0.038410481065511703, 0.0025528273545205593, -0.004377412144094706, -0.00675099715590477, 0.006689084693789482, 0.0026730282697826624, 0.01695932261645794, -0.022455744445323944, -0.014170611277222633, 0.018368937075138092, 0.05317522585391998, -0.019511310383677483, -0.04786750674247742, 0.011356540024280548, 0.036167509853839874, -0.018453096970915794, -0.013108968734741211, -0.003705380717292428, 0.0031310259364545345, 0.01957552507519722, 0.022996235638856888, -0.04692375287413597, 0.010523623786866665, 0.0151978749781847, 0.009586344473063946, -0.009694855660200119, 0.06290755420923233, -0.020792188122868538, -0.045424968004226685, -0.15302127599716187, -0.0014960793778300285, 0.016392981633543968, -0.007223495282232761, -0.012082898057997227, -0.02785315550863743, 0.02535806968808174, 0.0009754220372997224, -0.018229473382234573, -0.00011553549120435491, -0.016246603801846504, 0.018446525558829308, 0.012868187390267849, 0.015233148820698261, 0.008403956890106201, -0.0035992716439068317, -0.042287036776542664, 0.018119484186172485, 0.0014472078764811158, 0.006772890686988831, -0.037261154502630234, 0.04294487088918686, -0.04072646424174309, 0.008717454969882965, 0.014839038252830505, 0.002356856595724821, -0.028509268537163734, -0.0587744303047657, -0.007323797792196274, -0.07020425796508789, -0.020460154861211777, 0.0013530286960303783, 0.010395233519375324, 0.03011184185743332, 0.021195784211158752, -0.010125909931957722, 0.019076786935329437, -0.025666862726211548, 0.012932072393596172, 0.003098204266279936, -0.02096043899655342, 0.030774418264627457, -0.007811218500137329, -0.0074181207455694675, 0.013342125341296196, 0.02827904373407364, 0.02116740681231022, -0.011440825648605824, -0.05386372283101082, 0.02591989003121853, -0.055587124079465866, -0.0024650339037179947, -0.10242019593715668, -0.009212649427354336, -0.013324497267603874, 0.0174571480602026, 0.004072034731507301, -0.015411539934575558, -0.052660007029771805, 0.03861368075013161, -0.015914669260382652, 0.028986552730202675, -0.014176104217767715, -0.029218900948762894, -0.007927855476737022, 0.01631328836083412, -0.048870109021663666, 0.03039037063717842, 0.04374617710709572, -0.04552233964204788, -0.04128720611333847, 0.01328893844038248, -0.01783362776041031, 0.021039430052042007, 0.020082302391529083, -0.0026240693405270576, 0.014590953476727009, 0.0021560601890087128, 0.015555495396256447, -0.0180243831127882, 0.00387466698884964, -0.04059351608157158, -0.017667148262262344, 0.013516885228455067, 0.06464220583438873, -0.0053078969940543175, 0.020719559863209724, 0.027941640466451645, 0.055165331810712814, -0.02014738693833351, -0.045160628855228424, -0.020908063277602196, -0.008618908002972603, 0.024866502732038498, -0.013092943467199802, 0.01663080044090748, 0.015296213328838348, -0.0397157222032547, -0.01881938986480236, -0.02327280305325985, 0.037480369210243225, 0.020219353958964348, 0.00946501549333334, 0.03273487836122513, -0.04057733342051506, 0.04875422641634941, 0.04313022643327713, -0.0031367503106594086, 0.01641390472650528, -0.0166687723249197, 0.0052114990539848804, -0.006858861073851585, -0.0009919543517753482, 0.011691082268953323, -0.026969704777002335, 0.027193116024136543, -0.026623165234923363, -0.009551705792546272, -0.0812152624130249, 0.01891920529305935, -0.012630821205675602, 0.03687284514307976, -0.02955334447324276, -0.04365872964262962, -0.06358622014522552, -0.004390461836010218, 0.016415808349847794, 0.013363885693252087, 0.015943197533488274, -0.04743891581892967, 0.029777981340885162, -0.023721372708678246, -0.017798207700252533, 0.010984212160110474, 0.014349219389259815, -0.008148356340825558, -0.02200779691338539, 0.056212782859802246, -0.030449382960796356, -0.038457322865724564, -0.03935403749346733, 0.012095058336853981, 0.007333962246775627, -0.010145832784473896, 0.04818291217088699, -0.006354400888085365, 0.014433770440518856, 0.013005473650991917, 0.022247975692152977, -0.043795693665742874, 0.03307293355464935, -0.01626189611852169, -0.03347942978143692, 0.030900413170456886, 0.01975036785006523, 0.0037053741980344057, 0.054877035319805145, 0.00715956324711442, -0.02806491032242775, 0.043243926018476486, 0.027810178697109222, -0.026569802314043045, -0.03132997825741768, -0.027514895424246788, -0.016161726787686348, 0.004605554975569248, -0.019134240224957466, 0.03173599764704704, -0.05500989779829979, -0.006796687841415405, 0.02597261592745781, -0.006349897477775812, -0.00793306715786457, 0.03255782648921013, 0.023534180596470833, 0.034416571259498596, 0.02472779154777527, 0.008446929045021534, 0.04494461789727211, 0.05273646116256714, 0.027852870523929596, 0.042726922780275345, -0.012875341810286045, 0.014557878486812115, -0.006294076796621084, 0.0563679076731205, 0.02326614409685135, 0.0435221828520298, -0.03701339662075043, -0.03432697057723999, 0.027957096695899963, 0.015457337722182274, 0.006810764316469431, 0.027425691485404968, 0.006900117266923189, -0.012477259151637554, 0.04869560897350311, -0.0059214429929852486, -0.015709098428487778, 0.03524148464202881, -0.03533501178026199, 0.006197778508067131, -0.02276379056274891, 0.005504221189767122, -0.005742272362112999, 0.00864323414862156, 0.01449623517692089, -0.08615878969430923, -0.019902745261788368, -0.012709731236100197, -0.042176391929388046, 0.006437498610466719, 0.0015244637615978718, 0.05173034593462944, -0.049557872116565704, 0.014568259008228779, 0.02895624376833439, 0.010240156203508377, -0.001294239773415029, -0.051832351833581924, 0.011399160139262676, 0.057161569595336914, 0.05856688693165779, 0.009896301664412022, 0.0004667540779337287, -0.0018993027042597532, -0.014930336736142635, -0.009480022825300694, -0.049976982176303864, -0.025915687903761864, 0.03031117282807827, -0.004909704439342022, -0.032247595489025116, 0.0592353381216526, 0.08491376042366028, -0.04929697513580322, -0.01218048669397831, -0.027961745858192444, -0.006770662497729063, 0.012578483670949936, 0.019840944558382034, 0.008873778395354748, 0.021609125658869743, 0.0015359801473096013]\n"
     ]
    }
   ],
   "source": [
    "emb = BGEEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "print(emb.get_emb(\"建筑结构的安全性检查包括哪些方面？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f9a445",
   "metadata": {},
   "source": [
    "当观察到 Embedding 正确给出了编码后的向量，我们这一模块的构建就完成了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878401f7",
   "metadata": {},
   "source": [
    "### 3. 实现文档预处理模块\n",
    "\n",
    "为了处理建筑文档，我们需要预先准备好文档读取模块。本系统假设所有建筑规范和标准已经转换为Markdown格式，便于后续的文本处理和分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317fa91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_documents(self, directory_path: str) -> List[str]:\n",
    "        documents = []\n",
    "        \n",
    "        for file_path in Path(directory_path).rglob('*.md'):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                    \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62252a8f",
   "metadata": {},
   "source": [
    "完成文档预处理模块的设置后，我们就可以采用下面的方法来加载建筑规范文档了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e6c0e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载了 1 个建筑规范文档\n"
     ]
    }
   ],
   "source": [
    "processor = DocumentProcessor()\n",
    "documents = processor.load_documents(\"./construction_standards\")\n",
    "print(f\"加载了 {len(documents)} 个建筑规范文档\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c7210",
   "metadata": {},
   "source": [
    "## 核心实现\n",
    "\n",
    "建筑文档审查系统的主要流程如下。首先，让我们来梳理一下建筑文档审查的工作流程，系统的一个核心思想在于，我们需要把用户提供的文档内容通过智能化的问询生成和知识引导检索来识别潜在的合规性问题。与传统RAG方法不同，我们的系统专门针对建筑领域的专业特点进行了优化，能够更准确地理解建筑规范要求，提供更可靠的审查建议。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5f361",
   "metadata": {},
   "source": [
    "### 动态语义知识分块\n",
    "\n",
    "在传统RAG流程中，文本通过设置固定的token数量划分文本区块。然而，固定token数量会在句子中间截断，导致信息缺失。为此，本系统使用基于建筑文本语义动态划分的方式，通过双重语义聚类的方式，完成考虑建筑语义连贯性的知识chunk划分。\n",
    "\n",
    "首先，将整个文档内容处理成单独句子序列 $S = \\{s_0, s_1, \\ldots, s_a\\}$。通过计算相邻句子间的语义差异度来识别潜在的语义边界：\n",
    "\n",
    "$$\\gamma_i = 1 - \\frac{s_{i-1} \\cdot s_i}{\\|s_{i-1}\\| \\|s_i\\|}$$\n",
    "\n",
    "基于语义差异度分布自动确定动态阈值：\n",
    "\n",
    "$$\\psi = \\text{Quantile}(\\Gamma, \\frac{a-p}{a})$$\n",
    "\n",
    "确保最终的分块既保持语义连贯性又满足长度约束：\n",
    "\n",
    "$$\\mathbb{E}[\\gamma_{\\text{intra}}] < \\mathbb{E}[\\gamma_{\\text{inter}}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbb0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class DynamicSemanticChunker:\n",
    "   def __init__(self, \n",
    "                embedding_model: str = \"BAAI/bge-m3\",\n",
    "                max_chunk_length: int = 512,\n",
    "                min_chunk_length: int = 50):\n",
    "       self.embedding_model = SentenceTransformer(\n",
    "           embedding_model,\n",
    "           cache_folder=\"./model_cache\"\n",
    "       )\n",
    "       self.max_chunk_length = max_chunk_length\n",
    "       self.min_chunk_length = min_chunk_length\n",
    "   \n",
    "   def split_text(self, text: str) -> Dict[str, str]:\n",
    "       sentences = self._split_into_sentences(text)\n",
    "       if len(sentences) == 0:\n",
    "           return {}\n",
    "       \n",
    "       total_tokens = sum(len(self.embedding_model.tokenizer.encode(s)) for s in sentences)\n",
    "       baseline_chunks = math.ceil(total_tokens / self.max_chunk_length)\n",
    "       alpha = (len(sentences) - baseline_chunks) / len(sentences)\n",
    "       \n",
    "       sentence_embeddings = self.embedding_model.encode(sentences)\n",
    "       gamma_values = self._compute_semantic_discrepancy(sentence_embeddings)\n",
    "       threshold = np.quantile(gamma_values, alpha) if len(gamma_values) > 0 and alpha > 0 else 0.5\n",
    "       \n",
    "       boundaries = self._identify_boundaries(gamma_values, threshold)\n",
    "       initial_chunks = self._create_initial_chunks(sentences, boundaries)\n",
    "       final_chunks = self._enforce_length_constraints(initial_chunks)\n",
    "       \n",
    "       chunks_dict = {}\n",
    "       for i, chunk in enumerate(final_chunks):\n",
    "           if chunk.strip():\n",
    "               chunk_id = f\"chunk-{i+1:03d}\"\n",
    "               chunks_dict[chunk_id] = chunk.strip()\n",
    "       \n",
    "       return chunks_dict\n",
    "   \n",
    "   def _split_into_sentences(self, text: str) -> List[str]:\n",
    "       sentence_pattern = r'[。！？；\\n]+'\n",
    "       sentences = re.split(sentence_pattern, text)\n",
    "       \n",
    "       cleaned_sentences = []\n",
    "       for sentence in sentences:\n",
    "           sentence = sentence.strip()\n",
    "           if len(sentence) > 5:\n",
    "               cleaned_sentences.append(sentence)\n",
    "       \n",
    "       return cleaned_sentences\n",
    "   \n",
    "   def _compute_semantic_discrepancy(self, embeddings: np.ndarray) -> List[float]:\n",
    "       gamma_values = []\n",
    "       \n",
    "       for i in range(1, len(embeddings)):\n",
    "           similarity = cosine_similarity(\n",
    "               embeddings[i-1].reshape(1, -1),\n",
    "               embeddings[i].reshape(1, -1)\n",
    "           )[0][0]\n",
    "           \n",
    "           gamma = 1 - similarity\n",
    "           gamma_values.append(gamma)\n",
    "       \n",
    "       return gamma_values\n",
    "   \n",
    "   def _identify_boundaries(self, gamma_values: List[float], threshold: float) -> List[int]:\n",
    "       boundaries = [0]\n",
    "       \n",
    "       for i, gamma in enumerate(gamma_values):\n",
    "           if gamma > threshold:\n",
    "               boundaries.append(i + 1)\n",
    "       \n",
    "       boundaries.append(len(gamma_values) + 1)\n",
    "       return sorted(set(boundaries))\n",
    "   \n",
    "   def _create_initial_chunks(self, sentences: List[str], boundaries: List[int]) -> List[str]:\n",
    "       chunks = []\n",
    "       \n",
    "       for i in range(len(boundaries) - 1):\n",
    "           start = boundaries[i]\n",
    "           end = boundaries[i + 1]\n",
    "           \n",
    "           chunk_sentences = sentences[start:end]\n",
    "           chunk_text = ' '.join(chunk_sentences)\n",
    "           chunks.append(chunk_text)\n",
    "       \n",
    "       return chunks\n",
    "   \n",
    "   def _enforce_length_constraints(self, chunks: List[str]) -> List[str]:\n",
    "       final_chunks = []\n",
    "       \n",
    "       for chunk in chunks:\n",
    "           chunk_tokens = len(self.embedding_model.tokenizer.encode(chunk))\n",
    "           \n",
    "           if chunk_tokens <= self.max_chunk_length:\n",
    "               if chunk_tokens >= self.min_chunk_length:\n",
    "                   final_chunks.append(chunk)\n",
    "           else:\n",
    "               split_chunks = self._split_overlong_chunk(chunk)\n",
    "               final_chunks.extend(split_chunks)\n",
    "       \n",
    "       return final_chunks\n",
    "   \n",
    "   def _split_overlong_chunk(self, chunk: str) -> List[str]:\n",
    "       sentences = re.split(r'[。！？；\\n]+', chunk)\n",
    "       sentences = [s.strip() for s in sentences if s.strip()]\n",
    "       \n",
    "       if not sentences:\n",
    "           return [chunk]\n",
    "       \n",
    "       result_chunks = []\n",
    "       current_chunk_sentences = []\n",
    "       current_tokens = 0\n",
    "       \n",
    "       for sentence in sentences:\n",
    "           if current_chunk_sentences:\n",
    "               temp_text = ' '.join(current_chunk_sentences + [sentence])\n",
    "           else:\n",
    "               temp_text = sentence\n",
    "               \n",
    "           temp_tokens = len(self.embedding_model.tokenizer.encode(temp_text))\n",
    "           \n",
    "           if temp_tokens > self.max_chunk_length and current_chunk_sentences:\n",
    "               chunk_text = ' '.join(current_chunk_sentences)\n",
    "               if len(self.embedding_model.tokenizer.encode(chunk_text)) >= self.min_chunk_length:\n",
    "                   result_chunks.append(chunk_text)\n",
    "               \n",
    "               current_chunk_sentences = [sentence]\n",
    "               current_tokens = len(self.embedding_model.tokenizer.encode(sentence))\n",
    "           else:\n",
    "               current_chunk_sentences.append(sentence)\n",
    "               current_tokens = temp_tokens\n",
    "       \n",
    "       if current_chunk_sentences:\n",
    "           chunk_text = ' '.join(current_chunk_sentences)\n",
    "           if len(self.embedding_model.tokenizer.encode(chunk_text)) >= self.min_chunk_length:\n",
    "               result_chunks.append(chunk_text)\n",
    "       \n",
    "       return result_chunks if result_chunks else [chunk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab99fff",
   "metadata": {},
   "source": [
    "### 建筑文档审查系统\n",
    "\n",
    "整体的审查过程如下图所示。系统获取需要审查的区域后，依据提示生成审查问题推荐，此部分也可供工程师进行相关问题输入或推荐问题选择，生成待审查问题。随后，系统通过生成式知识引导检索框架，依据审查问题在所建文本知识库中检索出相应的知识参考。最终，依据检索的部分与审查原文，进行问题分析与审查修正，完成最终的审查流程。\n",
    "\n",
    "![picture](images/pic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f156b1d",
   "metadata": {},
   "source": [
    "#### 审查问题生成\n",
    "\n",
    "在文档审查流程中，系统引入了双阶段Prompt工程驱动的智能化问询生成机制，旨在对建筑施工交底文档进行预见性分析与风险挖掘，实现对文档潜在问题的高效、精准定位。\n",
    "\n",
    "阶段1为待查文档主旨目标解构，模型被指示从文本中提炼核心事件、关键技术、工艺流程等要素，结构化地总结文档的核心内容，由此界定本次审查的靶向目标，为后续的精细化问询奠定基础。阶段2为多维度风险探测与定制化问询生成，基于第一阶段提炼的核心要素，通过few-shot等方式引导 LLM 从合规性、安全性、可操作性等多维度对文档进行风险探测。Prompt 指示模型围绕潜在的限制条件、操作流程、以及可能存在的合规性隐患等方面，进行细粒度、多角度的审查提问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d283d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CORE_COMPONENTS_PROMPT = \"\"\"\n",
    "Task: Extract key information components from the construction document below. Focus on technical requirements, construction methods, and compliance-related elements.\n",
    "\n",
    "Please identify:\n",
    "1. Technical specifications and standards\n",
    "2. Construction techniques and processes  \n",
    "3. Quality requirements and restrictions\n",
    "\n",
    "Input: {document_chunk}\n",
    "\n",
    "Please provide a concise summary in English:\n",
    "\"\"\"\n",
    "\n",
    "REVIEW_QUERIES_PROMPT = \"\"\"\n",
    "Task: Generate 3-5 specific review questions based on the construction document and extracted components. These questions should help identify potential compliance issues by retrieving relevant construction codes and standards.\n",
    "\n",
    "Document: {document_chunk}\n",
    "Key components: {core_components}\n",
    "\n",
    "Generate review questions (one question per line):\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "5.\n",
    "\"\"\"\n",
    "\n",
    "def generate_review_queries(llm, document_chunk: str) -> List[str]:\n",
    "    core_prompt = CORE_COMPONENTS_PROMPT.format(document_chunk=document_chunk)\n",
    "    core_response = llm.predict(core_prompt)\n",
    "    \n",
    "    queries_prompt = REVIEW_QUERIES_PROMPT.format(\n",
    "        document_chunk=document_chunk,\n",
    "        core_components=core_response\n",
    "    )\n",
    "    queries_response = llm.predict(queries_prompt)\n",
    "    \n",
    "    queries = []\n",
    "    lines = queries_response.strip().split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        line = re.sub(r'^\\d+[\\.\\)]\\s*', '', line)\n",
    "        line = re.sub(r'^\\*\\s*', '', line)\n",
    "        line = re.sub(r'^-\\s*', '', line) \n",
    "        \n",
    "        if line and len(line) > 5:\n",
    "            queries.append(line)\n",
    "    \n",
    "    return queries[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b93451",
   "metadata": {},
   "source": [
    "#### 知识引导生成式检索\n",
    "\n",
    "系统的核心创新在于知识引导的检索框架，整个过程分为三个关键步骤。步骤1为句子级编码，主要负责输入查询句子的初始表示学习，计算查询与知识库chunks间的句子级相似度分数。步骤2为知识引导检索，进一步从查询中提取关键信息，利用这些信息结合文档长度自适应加权等机制，对每个知识库chunk进行更详细的评分。步骤3为重排序与增强，使用大语言模型对步骤2检索的结果进行进一步重排序，并利用精炼的知识来增强原始查询。\n",
    "![picture](images/pic2.png)\n",
    "\n",
    "首先建立专门针对建筑领域文本分析的深度提取模块，集成领域预训练BERT进行上下文编码，结合双向LSTM进行建筑法规依赖建模。建立三级重要性分类层次：max（最高）、mid（中等）、lit（字面）优先级。本项目直接通过大语言模型进行关键信息提取，如果需要更精准的效果，可以自行训练BERT模型进行专门的关键信息提取。\n",
    "![picture](images/pic3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc88609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "KEY_INFO_EXTRACTION_PROMPT = \"\"\"\n",
    "Your task is to extract key information from the query with three different priority levels:\n",
    "\n",
    "Maximum priority (max): The most important core concepts or entities\n",
    "Medium priority (mid): Important modifiers or qualifying conditions  \n",
    "Literal priority (lit): Specific values, standards or specifications\n",
    "\n",
    "Query: {query}\n",
    "max:\n",
    "mid:\n",
    "lit:\n",
    "\"\"\"\n",
    "\n",
    "class KeyInfoExtractor:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def extract_key_info(self, query: str) -> Dict[str, Tuple[str, float]]:\n",
    "        prompt = KEY_INFO_EXTRACTION_PROMPT.format(query=query)\n",
    "        response = self.llm.predict(prompt)\n",
    "        \n",
    "        lines = response.strip().split('\\n')\n",
    "        key_info = {}\n",
    "        weights = {'max': 0.5, 'mid': 0.3, 'lit': 0.2}\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith('max:'):\n",
    "                key_info['max'] = (line[4:].strip(), weights['max'])\n",
    "            elif line.startswith('mid:'):\n",
    "                key_info['mid'] = (line[4:].strip(), weights['mid'])\n",
    "            elif line.startswith('lit:'):\n",
    "                key_info['lit'] = (line[4:].strip(), weights['lit'])\n",
    "        \n",
    "        return key_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322faa01",
   "metadata": {},
   "source": [
    "#### 文档长度自适应因子\n",
    "\n",
    "在知识引导检索过程中，文档长度自适应因子用于调整不同长度文档的权重分配，确保长短文档都能得到公平的评分机会。该因子的计算考虑了当前文档chunk的长度与平均文档长度的关系。\n",
    "\n",
    "$$\\Lambda_{\\text{DL}} = \\frac{\\overline{|k|} + |k_j|}{2\\overline{|k|}}$$\n",
    "\n",
    "其中 $|k_j|$ 表示当前文档chunk的长度，$\\overline{|k|}$ 表示平均文档长度。通过这种归一化处理，可以避免因文档长度差异导致的评分偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "290be1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_length_factor(chunk_length: int, avg_length: int = 100) -> float:\n",
    "    lambda_dl = (avg_length + chunk_length) / (2 * avg_length)\n",
    "    return lambda_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602cb08",
   "metadata": {},
   "source": [
    "#### 术语重要性计算\n",
    "\n",
    "术语重要性指标衡量术语在文档中的显著程度，结合术语频率和文档长度自适应因子，能够更准确地评估术语在当前文档中的重要性。计算公式考虑了术语频率的非线性增长特性。\n",
    "\n",
    "$$\\text{Sign}(t_{e_i}^\\tau, k_j) = \\frac{2 \\cdot f(t_{e_i}^\\tau, k_j) \\cdot \\Lambda_{\\text{DL}}}{f(t_{e_i}^\\tau, k_j) + 1}$$\n",
    "\n",
    "其中 $f(t_{e_i}^\\tau, k_j)$ 表示术语在文档chunk中的出现频率，$\\Lambda_{\\text{DL}}$ 为文档长度自适应因子。这种计算方式能够防止高频术语过度影响评分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba8bcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_term_significance(term_freq: int, doc_length_factor: float) -> float:\n",
    "    significance = (2 * term_freq * doc_length_factor) / (term_freq + 1)\n",
    "    return significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf4a3b",
   "metadata": {},
   "source": [
    "#### 术语稀有度计算\n",
    "\n",
    "术语稀有度用于衡量术语在整个知识库中的稀缺程度，稀有度越高的术语在检索中的权重越大。计算采用了改进的IDF公式，增加了平滑处理以避免零除问题。\n",
    "\n",
    "$\\text{Rarity}(t_{e_i}^\\tau) = \\log\\left(\\frac{D - \\text{df}(t_{e_i}^\\tau) + 0.5}{\\text{df}(t_{e_i}^\\tau) + 0.5} + 1\\right)$\n",
    "\n",
    "其中 $D$ 表示文档总数，$\\text{df}(t_{e_i}^\\tau)$ 表示包含该术语的文档数量。加一操作确保了对数值始终为正数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6dc2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_term_rarity(doc_freq: int, total_docs: int) -> float:\n",
    "    rarity = np.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1)\n",
    "    return rarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ec6e7",
   "metadata": {},
   "source": [
    "#### 连贯性指数评估\n",
    "\n",
    "连贯性指数反映术语在文档中的分布连贯性，通过滑动窗口技术分析术语在文档中的局部分布情况。连贯性高的术语往往在文档的特定区域集中出现，表明其与文档主题的强相关性。\n",
    "\n",
    "$$\\text{CI}(t_{e_i}^\\tau, k_j) = \\max_{w \\in W, \\, t \\in w} \\frac{\\sum I(t = t_{e_i}^\\tau) \\cdot |w|}{|k_j|}$$\n",
    "\n",
    "其中 $W$ 表示文档中的滑动窗口集合，$I(t = t_{e_i}^\\tau)$ 为指示函数，当窗口中包含该术语时为1，否则为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33644f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_index(term: str, chunk: str, window_size: int = 50) -> float:\n",
    "    chunk_tokens = chunk.lower().split()\n",
    "    chunk_length = len(chunk_tokens)\n",
    "    \n",
    "    if chunk_length == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    max_coherence = 0.0\n",
    "    \n",
    "    for i in range(0, chunk_length - window_size + 1, 10):\n",
    "        window = chunk_tokens[i:i + window_size]\n",
    "        term_count = window.count(term.lower())\n",
    "        \n",
    "        if term_count > 0:\n",
    "            coherence = (term_count * window_size) / chunk_length\n",
    "            max_coherence = max(max_coherence, coherence)\n",
    "    \n",
    "    return max_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494fe967",
   "metadata": {},
   "source": [
    "#### 评分融合与检索\n",
    "\n",
    "将句子级相似度评分与知识级评分进行融合，形成最终的文档相关性评分。融合过程采用加权平均的方式，平衡参数λ控制两种评分方式的重要性。\n",
    "\n",
    "$\\Phi = \\lambda \\Phi(\\mathcal{K}) + (1 - \\lambda) \\Phi(\\mathcal{S})$\n",
    "\n",
    "其中 $\\lambda$ 为平衡参数，$\\Phi(\\mathcal{K})$ 为知识级评分，$\\Phi(\\mathcal{S})$ 为句子级评分。通过调整λ值，可以控制系统更偏向语义相似还是知识匹配。当λ=0时，系统完全依赖句子级语义相似度；当λ=1时，系统完全依赖知识匹配评分；λ=0.5时，两种评分方式权重相等。在建筑文档审查场景中，通常设置λ=0.5以平衡专业知识匹配和语义理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66d44da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "class GKGRRetriever:\n",
    "    def __init__(self, \n",
    "                 knowledge_base: List[str],\n",
    "                 embedding_model,\n",
    "                 key_info_extractor: KeyInfoExtractor,\n",
    "                 llm,\n",
    "                 config: Dict[str, Any] = None):\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.embedding_model = embedding_model\n",
    "        self.key_info_extractor = key_info_extractor\n",
    "        self.llm = llm\n",
    "        \n",
    "        default_config = {\n",
    "            \"lambda_param\": 0.5,\n",
    "            \"top_k\": 5,\n",
    "            \"rerank_enabled\": True,\n",
    "            \"query_expansion\": True,\n",
    "            \"similarity_threshold\": 0.1\n",
    "        }\n",
    "        self.config = {**default_config, **(config or {})}\n",
    "        \n",
    "        self.kb_embeddings = self._precompute_embeddings()\n",
    "    \n",
    "    def _precompute_embeddings(self) -> np.ndarray:\n",
    "        embeddings = self.embedding_model.encode(self.knowledge_base, show_progress_bar=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def retrieve_with_scores(self, query: str) -> List[Tuple[str, float, Dict[str, float]]]:\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        sentence_scores = cosine_similarity(\n",
    "            query_embedding.reshape(1, -1), \n",
    "            self.kb_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        key_info = self.key_info_extractor.extract_key_info(query)\n",
    "        knowledge_scores = self._compute_knowledge_scores(key_info)\n",
    "        \n",
    "        final_scores = []\n",
    "        for i in range(len(self.knowledge_base)):\n",
    "            norm_sent = sentence_scores[i]\n",
    "            norm_know = knowledge_scores[i] / max(knowledge_scores) if max(knowledge_scores) > 0 else 0\n",
    "            \n",
    "            final_score = (self.config[\"lambda_param\"] * norm_know + \n",
    "                          (1 - self.config[\"lambda_param\"]) * norm_sent)\n",
    "            final_scores.append(final_score)\n",
    "        \n",
    "        results_with_scores = []\n",
    "        for i, final_score in enumerate(final_scores):\n",
    "            if final_score > self.config[\"similarity_threshold\"]:\n",
    "                score_details = {\n",
    "                    \"sentence_score\": float(sentence_scores[i]),\n",
    "                    \"knowledge_score\": float(knowledge_scores[i]),\n",
    "                    \"final_score\": float(final_score)\n",
    "                }\n",
    "                results_with_scores.append((self.knowledge_base[i], final_score, score_details))\n",
    "        \n",
    "        results_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results_with_scores[:self.config[\"top_k\"]]\n",
    "    \n",
    "    def _compute_knowledge_scores(self, key_info: Dict[str, Tuple[str, float]]) -> List[float]:\n",
    "        scores = []\n",
    "        avg_length = sum(len(chunk.split()) for chunk in self.knowledge_base) / len(self.knowledge_base)\n",
    "        \n",
    "        for chunk in self.knowledge_base:\n",
    "            chunk_score = 0.0\n",
    "            chunk_tokens = chunk.lower().split()\n",
    "            chunk_length = len(chunk_tokens)\n",
    "            \n",
    "            lambda_dl = compute_document_length_factor(chunk_length, avg_length)\n",
    "            \n",
    "            for priority, (info_text, weight) in key_info.items():\n",
    "                if not info_text.strip():\n",
    "                    continue\n",
    "                \n",
    "                terms = info_text.lower().split()\n",
    "                for term in terms:\n",
    "                    if term in chunk_tokens:\n",
    "                        tf = chunk_tokens.count(term)\n",
    "                        \n",
    "                        significance = compute_term_significance(tf, lambda_dl)\n",
    "                        \n",
    "                        segments_with_term = sum(1 for kb_chunk in self.knowledge_base \n",
    "                                                if term in kb_chunk.lower())\n",
    "                        rarity = compute_term_rarity(segments_with_term, len(self.knowledge_base))\n",
    "                        \n",
    "                        coherence = compute_coherence_index(term, chunk)\n",
    "                        \n",
    "                        term_score = significance * rarity * (1 + coherence) * weight\n",
    "                        chunk_score += term_score\n",
    "            \n",
    "            scores.append(chunk_score)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def retrieve(self, query: str) -> Tuple[List[str], str]:\n",
    "        results_with_scores = self.retrieve_with_scores(query)\n",
    "        \n",
    "        documents = [doc for doc, _, _ in results_with_scores]\n",
    "        \n",
    "        if self.config[\"rerank_enabled\"] and len(documents) > 1:\n",
    "            documents = self._llm_rerank(query, documents)\n",
    "        \n",
    "        augmented_query = query\n",
    "        if self.config[\"query_expansion\"]:\n",
    "            augmented_query = self._augment_query(query, documents[:3])\n",
    "        \n",
    "        return documents, augmented_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e7375",
   "metadata": {},
   "source": [
    "#### 重排序优化\n",
    "\n",
    "系统使用大语言模型对检索结果进行进一步重排序，通过LLM的语义理解能力优化文档的相关性排序。重排序过程中，系统会构造包含查询和候选文档的提示，要求LLM根据相关性对文档进行重新排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ae7c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _llm_rerank(self, query: str, documents: List[str]) -> List[str]:\n",
    "    if len(documents) <= 1:\n",
    "        return documents\n",
    "    \n",
    "    rerank_prompt = f\"\"\"\n",
    "Task: A list of documents is shown below. Each document has a number next to it. A question is also provided. Your task is to return the numbers of ALL documents in order of relevance from MOST to LEAST relevant. MUST include EVERY document number exactly once.\n",
    "\n",
    "Example format:\n",
    "    Document 1: <document 1>\n",
    "    Document 2: <document 2>\n",
    "    Document 3: <document 3>\n",
    "    Question: <question>\n",
    "    Answer: 3,1,2\n",
    "\n",
    "Now here are the actual documents and question.\n",
    "\n",
    "\"\"\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        rerank_prompt += f\"Document {i+1}: {doc[:150]}...\\n\"\n",
    "    \n",
    "    rerank_prompt += f\"Question: {query}\\nAnswer:\"\n",
    "    \n",
    "    try:\n",
    "        response = self.llm.predict(rerank_prompt)\n",
    "        order_nums = [int(x.strip()) - 1 for x in response.split(',') \n",
    "                     if x.strip().isdigit() and 0 <= int(x.strip()) - 1 < len(documents)]\n",
    "        \n",
    "        reranked = [documents[i] for i in order_nums if i < len(documents)]\n",
    "        \n",
    "        # 添加遗漏的文档\n",
    "        used_indices = set(order_nums)\n",
    "        for i, doc in enumerate(documents):\n",
    "            if i not in used_indices:\n",
    "                reranked.append(doc)\n",
    "        \n",
    "        return reranked[:len(documents)]\n",
    "    except:\n",
    "        return documents\n",
    "    \n",
    "GKGRRetriever._llm_rerank = _llm_rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee71c1",
   "metadata": {},
   "source": [
    "#### 查询增强\n",
    "\n",
    "同时系统还会利用检索到的知识来增强原始查询，生成更具体、更详细的查询用于进一步检索。查询增强通过分析检索结果的上下文信息，识别查询中可能遗漏的关键概念和术语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce6e9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _augment_query(self, original_query: str, top_results: List[str]) -> str:\n",
    "    if not top_results:\n",
    "        return original_query\n",
    "    \n",
    "    document_list = \"\"\n",
    "    for i, doc in enumerate(top_results):\n",
    "        document_list += f\"Document {i+1}: {doc[:100]}...\\n\"\n",
    "    \n",
    "    augment_prompt = f\"\"\"\n",
    "Task: Your task is to generate a detailed answer to the question by synthesizing information from ALL provided documents. Prioritize relevance, cite document numbers, and structure your response as follows:\n",
    "\n",
    "Question: {original_query}\n",
    "{document_list}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        augmented = self.llm.predict(augment_prompt)\n",
    "        return augmented.strip()\n",
    "    except:\n",
    "        return original_query\n",
    "\n",
    "GKGRRetriever._augment_query = _augment_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c783319",
   "metadata": {},
   "source": [
    "#### 偏差检测分析\n",
    "\n",
    "在先期知识增强检索阶段获取领域知识后，系统随即进入误差辨析模块。该模块基于检索得到的知识参考，并结合预设的审阅问题，对原文进行细致的偏差检测与评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d84e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorAnalyzer:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def analyze_errors(self, document_chunk: str, query: str, retrieved_knowledge: List[str]) -> Dict[str, Any]:\n",
    "        \n",
    "        analysis_prompt = f\"\"\"\n",
    "Task: Your task is to conduct an error analysis on a given review document, based on a provided review query and relevant reference specifications. This analysis MUST strictly adhere to the provided reference and focus specifically on reviewing and analyzing the original descriptive sections within the review document.\n",
    "\n",
    "Review document: {document_chunk}\n",
    "Query: {query}\n",
    "Reference: {chr(10).join([f\"{i+1}. {ref}\" for i, ref in enumerate(retrieved_knowledge)])}\n",
    "Analysis:\n",
    "\"\"\"\n",
    "        \n",
    "        analysis = self.llm.predict(analysis_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"analysis\": analysis,\n",
    "            \"reference_support\": retrieved_knowledge\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421c272",
   "metadata": {},
   "source": [
    "#### 修订建议生成\n",
    "\n",
    "误差辨析模块完成后，系统将输出标记偏差区域以及相关知识佐证。随后，系统进入修订策略生成模块。该模块依据误差分析结果和知识参考，对标记区域进行针对性的修订建议生成，最终实现对原文的知识驱动型自动修正。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d4ed81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevisionGenerator:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def generate_revisions(self, document_chunk: str, analysis: Dict[str, Any]) -> Dict[str, str]:     \n",
    "        revision_prompt = f\"\"\"\n",
    "Task: Your task is to review and revise the provided document based on the given analysis and corresponding reference specifications. STRICT adherence to the provided reference specifications is required. If the review document aligns with the analysis and reference specifications WITHOUT discrepancies, revision is not necessary.\n",
    "\n",
    "Review document: {document_chunk}\n",
    "Analysis: {analysis['analysis']}\n",
    "Reference: {chr(10).join([f\"- {ref}\" for ref in analysis['reference_support']])}\n",
    "Revision:\n",
    "\"\"\"\n",
    "        \n",
    "        revision = self.llm.predict(revision_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"original_text\": document_chunk,\n",
    "            \"revision_suggestions\": revision,\n",
    "            \"modified_regions\": analysis.get(\"error_regions\", []),\n",
    "            \"confidence\": self._calculate_confidence(analysis)\n",
    "        }\n",
    "    \n",
    "    def _calculate_confidence(self, analysis: Dict[str, Any]) -> float:\n",
    "        ref_count = len(analysis.get(\"reference_support\", []))\n",
    "        error_count = len(analysis.get(\"error_regions\", []))\n",
    "        \n",
    "        confidence = min(0.9, 0.5 + (ref_count * 0.1) + (error_count * 0.05))\n",
    "        return confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2f9f5",
   "metadata": {},
   "source": [
    "#### 完整审查流程\n",
    "\n",
    "将上述所有模块整合，形成完整的文档审查流程。系统首先生成审查问题，然后进行知识引导检索，接着执行错误分析，最后生成修订建议。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7378cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_review_process(document_chunk: str, \n",
    "                          gkgr_framework: GKGRRetriever, \n",
    "                          error_analyzer: ErrorAnalyzer,\n",
    "                          revision_generator: RevisionGenerator) -> Dict[str, Any]:    \n",
    "    review_queries = generate_review_queries(gkgr_framework.llm, document_chunk)\n",
    "    \n",
    "    results = {}\n",
    "    for query in review_queries[:3]:\n",
    "        retrieved_docs, augmented_query = gkgr_framework.retrieve(query)\n",
    "        \n",
    "        knowledge_refs = retrieved_docs\n",
    "        analysis = error_analyzer.analyze_errors(document_chunk, query, knowledge_refs)\n",
    "        \n",
    "        revision = revision_generator.generate_revisions(document_chunk, analysis)\n",
    "        \n",
    "        results[query] = {\n",
    "            \"retrieved_knowledge\": retrieved_docs,\n",
    "            \"augmented_query\": augmented_query,\n",
    "            \"analysis\": analysis,\n",
    "            \"revision\": revision\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6173a",
   "metadata": {},
   "source": [
    "至此，我们就完成了建筑文档智能审查系统的核心实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ff27c",
   "metadata": {},
   "source": [
    "## 实际应用示例\n",
    "\n",
    "让我们通过一个完整的示例来展示系统的使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba3b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "审查问题: Is the minimum concrete compressive strength of C25 verified by test reports and does it meet the design requirements specified in standards like GB 50010?\n",
      "修订建议: 根据提供的分析报告和参考规范GB 50496-2018《大体积混凝土施工标准》，原审查文档存在重大不符合项，特别是关于混凝土强度验收指标和养护要求。审查文档未体现大体积混凝土施工的特殊性，与参考规范存在严重偏差。必须进行修订。\n",
      "\n",
      "**修订后的文档：**\n",
      "\n",
      "钢筋混凝土柱（大体积混凝土）的施工应符合以下要求：\n",
      "1.  混凝土强度等级应符合结构设计要求（如基于GB 50010），并宜采用60d或90d龄期的强度指标作为配合比设计和验收依据。应提供法定检测机构出具的混凝土抗压强度检验报告进行验证。\n",
      "2.  钢筋保护层厚度应符合设计要求及GB 50496等现行有关标准的规定。\n",
      "3.  混凝土浇筑应连续进行，采用整体分层或推移式方法施工。层间间歇时间不应大于混凝土的初凝时间。\n",
      "4.  混凝土应采用保温保湿养护，专人负责，持续时间不宜少于14d，并应经常检查保持混凝土表面湿润。保温覆盖层拆除应分层逐步进行，当混凝土表面温度与环境最大温差小于20℃时，方可全部拆除。\n",
      "\n",
      "**修订说明：**\n",
      "1.  **第1条**：根据分析报告指出的“未指定强度龄期”和“未要求验证”的关键错误，严格依据GB 50496-2018第4.3.1条，将强度要求修订为鼓励采用60d或90d龄期强度，并明确要求通过试验报告进行验证。\n",
      "2.  **第2条**：保护层厚度首先应满足设计要求，参考规范中未对保护层厚度做统一规定，故修订为更具通用性的表述。\n",
      "3.  **第3条**：参考GB 50496-2018第5.4.1条，将“间歇时间不超过1小时”的绝对化表述，修订为更科学的“不应大于混凝土的初凝时间”，并补充了大体积混凝土推荐的浇筑方法。\n",
      "4.  **第4条**：根据分析报告指出的养护要求不具体的问题，依据GB 50496-2018第5.5.1条，将“保持湿润”的普通养护要求，修订为针对大体积混凝土的“保温保湿养护”，并明确了养护时间、责任人以及保温层的拆除条件。\n",
      "--------------------------------------------------\n",
      "审查问题: Is the 25mm clear cover to reinforcement for the columns confirmed through inspection and does it comply with the minimum cover requirements for durability in GB 50010 based on the environmental category?\n",
      "修订建议: 钢筋混凝土柱的施工应符合以下要求：\n",
      "1. 混凝土强度等级不低于C25\n",
      "2. 钢筋的混凝土保护层最小厚度应符合设计规定及现行国家标准《混凝土结构设计规范》GB 50010中关于环境类别的要求，并应通过现场检测进行确认\n",
      "3. 混凝土浇筑应连续进行，间歇时间不超过1小时\n",
      "4. 养护期间应保持混凝土表面湿润，且保湿养护持续时间不宜少于14d\n",
      "\n",
      "**修订说明：**\n",
      "根据分析意见及GB 50496-2018标准要求，对原文档第2条关于钢筋保护层厚度的内容进行了修订。原条款仅规定了单一厚度，未考虑环境类别差异且未提及验收标准，不符合GB 50010的设计原则。修订后条款明确了保护层厚度应遵循设计规定和GB 50010标准，并增加了通过现场检测进行确认的要求，以满足施工质量控制和验收的需要。其余条款符合大体积混凝土施工标准的相关规定，予以保留。\n",
      "--------------------------------------------------\n",
      "审查问题: Is there a documented continuous concrete pouring sequence and method statement to ensure the maximum 1-hour间歇时间 between batches is not exceeded, as required to prevent cold joints?\n",
      "修订建议: 钢筋混凝土柱的施工应符合以下要求：\n",
      "1. 混凝土强度等级不低于C25\n",
      "2. 钢筋保护层厚度为25mm\n",
      "3. 混凝土浇筑应连续进行，层间间歇时间不应大于混凝土初凝时间（通过试验确定）\n",
      "4. 养护期间应保持混凝土表面湿润\n",
      "\n",
      "---\n",
      "**改写说明**：\n",
      "- **修正间歇时间要求**：将“间歇时间不超过1小时”改为“层间间歇时间不应大于混凝土初凝时间（通过试验确定）”，以严格遵循《大体积混凝土施工标准》GB 50496-2018中关于间歇时间应根据混凝土初凝时间试验确定、不宜简单设定固定时长的规定。\n",
      "- **保持其他条款一致**：对强度等级、保护层厚度及养护要求等无争议内容予以保留，未做变动。\n",
      "\n",
      "如果您有其他风格或用途上的需求，我可以进一步调整文本表达。\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "embedding = BGEEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "key_extractor = KeyInfoExtractor(llm)\n",
    "\n",
    "# 从markdown文档构建知识库\n",
    "processor = DocumentProcessor()\n",
    "documents = processor.load_documents(\"./construction_standards\")\n",
    "\n",
    "# 对文档进行动态语义分块\n",
    "chunker = DynamicSemanticChunker()\n",
    "knowledge_base = []\n",
    "for doc in documents:\n",
    "    chunks = chunker.split_text(doc)\n",
    "    knowledge_base.extend(chunks.values())\n",
    "\n",
    "# 初始化检索器\n",
    "gkgr_retriever = GKGRRetriever(\n",
    "    knowledge_base=knowledge_base,\n",
    "    embedding_model=embedding,\n",
    "    key_info_extractor=key_extractor,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# 初始化分析器\n",
    "error_analyzer = ErrorAnalyzer(llm)\n",
    "revision_generator = RevisionGenerator(llm)\n",
    "\n",
    "# 待审查的文档内容\n",
    "sample_document = \"\"\"\n",
    "钢筋混凝土柱的施工应符合以下要求：\n",
    "1. 混凝土强度等级不低于C25\n",
    "2. 钢筋保护层厚度为25mm\n",
    "3. 混凝土浇筑应连续进行，间歇时间不超过1小时\n",
    "4. 养护期间应保持混凝土表面湿润\n",
    "\"\"\"\n",
    "\n",
    "# 执行审查\n",
    "result = complete_review_process(\n",
    "    sample_document, \n",
    "    gkgr_retriever, \n",
    "    error_analyzer, \n",
    "    revision_generator\n",
    ")\n",
    "\n",
    "# 查看审查结果\n",
    "for query, analysis in result.items():\n",
    "    print(f\"审查问题: {query}\")\n",
    "    print(f\"修订建议: {analysis['revision']['revision_suggestions']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96070e33",
   "metadata": {},
   "source": [
    "## 扩展性说明\n",
    "\n",
    "系统可以通过更换知识库轻松适应其他领域。对于特定企业或项目，可以通过微调关键信息提取模型来提升准确性。在性能优化方面，使用动态语义分块可以提升检索质量，预计算并缓存知识库嵌入以提升检索速度，对于大量文档可使用批量处理模式，根据具体应用场景调整λ参数和top-k值。\n",
    "\n",
    "## 写在最后\n",
    "\n",
    "恭喜你阅读完此文，你已经充分了解了如何实现一个建筑文档智能审查系统以及其背后的思考。这个系统展示了如何将动态语义分块、知识引导检索和大语言模型有机结合，为建筑行业的文档审查提供了一个实用的解决方案。\n",
    "\n",
    "虽然当前系统已经取得了不错的效果，但仍有改进空间。全局关联增强方面，当前基于文本块的检索可以进一步结合知识图谱等技术。多模态支持方面，未来可以扩展支持CAD图纸、施工图等视觉信息。实时更新方面，支持知识库的增量更新和动态维护。个性化定制方面，根据不同企业和项目特点进行系统定制。\n",
    "\n",
    "读者们可以运行项目中的示例代码，体验完整的建筑文档智能审查流程。我们相信这个系统不仅能够提升审查效率，更能为建筑行业的数字化转型贡献力量。\n",
    "\n",
    "## 致谢\n",
    "\n",
    "本项目的开发过程中，我们深入研究了建筑工程领域的专业知识和最新的自然语言处理技术。特别感谢建筑行业专家提供的宝贵建议，以及开源社区在技术实现方面的支持。项目代码实现参考了LlamaIndex、Transformers等优秀开源项目的设计理念。\n",
    "\n",
    "需要说明的是，本项目专门针对建筑施工领域的文档审查场景进行了深度优化。如果您需要处理其他领域的文档，建议根据具体需求对系统进行相应调整。\n",
    "\n",
    "## 源码获取\n",
    "\n",
    "本项目的源码以及实例数据存放在 [GitHub 仓库](https://github.com/Hongru0306/CDDRS)。\n",
    "\n",
    "## 引用\n",
    "\n",
    "如果您在研究中使用了本项目的成果，请按如下方式引用：\n",
    "\n",
    "```bibtex\n",
    "@article{XIAO2025103618,\n",
    "  title = {Generative knowledge-guided review system for construction disclosure documents},\n",
    "  journal = {Advanced Engineering Informatics},\n",
    "  volume = {68},\n",
    "  pages = {103618},\n",
    "  year = {2025},\n",
    "  issn = {1474-0346},\n",
    "  doi = {https://doi.org/10.1016/j.aei.2025.103618},\n",
    "  url = {https://www.sciencedirect.com/science/article/pii/S1474034625005117},\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
