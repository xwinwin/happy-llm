import torch
from modelscope import AutoModelForCausalLM, AutoTokenizer

def test_decoding_strategies():
    """
    æµ‹è¯•ä¸‰ç§è§£ç ç­–ç•¥ï¼šè´ªå©ªè§£ç ã€éšæœºé‡‡æ ·ã€æŸæœç´¢
    """
    model_id = "../model/kmno4zx/happy-llm-215M-sft/"

    print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œtokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, device_map="cpu").eval()

    # æµ‹è¯•prompt
    test_prompt = "è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"},
        {"role": "user", "content": test_prompt}
    ]

    # å‡†å¤‡è¾“å…¥
    input_ids = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    input_ids = tokenizer(input_ids).data['input_ids']
    x = (torch.tensor(input_ids, dtype=torch.long)[None, ...]).to(model.device)

    print(f"æµ‹è¯•prompt: {test_prompt}")
    print(f"è¾“å…¥tokenæ•°é‡: {len(input_ids)}")
    print("=" * 60)

    # æµ‹è¯•1: è´ªå©ªè§£ç  (Greedy Search)
    print("ğŸ” æµ‹è¯•1: è´ªå©ªè§£ç  (Greedy Search)")
    print("å‚æ•°: do_sample=False, num_beams=1, temperature=0.0")
    print("ç‰¹ç‚¹: æ¯æ­¥é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„tokenï¼Œç»“æœç¡®å®šï¼Œé€Ÿåº¦å¿«")

    with torch.no_grad():
        greedy_output = model.generate_super(
            x,
            stop_id=tokenizer.eos_token_id,
            max_new_tokens=50,
            temperature=0.0,
            do_sample=False,
            num_beams=1
        )
        greedy_response = tokenizer.decode(greedy_output[0].tolist(), skip_special_tokens=True)

    print(f"è´ªå©ªè§£ç ç»“æœ: {greedy_response}")
    print()

    # æµ‹è¯•2: éšæœºé‡‡æ · (Random Sampling)
    print("ğŸ² æµ‹è¯•2: éšæœºé‡‡æ · (Random Sampling)")
    print("å‚æ•°: do_sample=True, num_beams=1, temperature=0.8, top_k=50")
    print("ç‰¹ç‚¹: åŸºäºæ¦‚ç‡åˆ†å¸ƒéšæœºé‡‡æ ·ï¼Œç»“æœå¤šæ ·ï¼Œåˆ›é€ æ€§é«˜")

    with torch.no_grad():
        # è¿è¡Œå¤šæ¬¡ä»¥å±•ç¤ºéšæœºæ€§
        for i in range(3):
            sampling_output = model.generate_super(
                x,
                stop_id=tokenizer.eos_token_id,
                max_new_tokens=50,
                temperature=0.8,
                top_k=50,
                do_sample=True,
                num_beams=1
            )
            sampling_response = tokenizer.decode(sampling_output[0].tolist(), skip_special_tokens=True)
            print(f"éšæœºé‡‡æ ·ç»“æœ {i+1}: {sampling_response}")

    print()

    # æµ‹è¯•3: æŸæœç´¢ (Beam Search)
    print("ğŸ”¦ æµ‹è¯•3: æŸæœç´¢ (Beam Search)")
    print("å‚æ•°: do_sample=False, num_beams=3, temperature=1.0")
    print("ç‰¹ç‚¹: ç»´æŠ¤å¤šæ¡å€™é€‰è·¯å¾„ï¼Œé€‰æ‹©æ€»æ¦‚ç‡æœ€é«˜çš„åºåˆ—ï¼Œè´¨é‡æ›´é«˜")

    with torch.no_grad():
        beam_output = model.generate_super(
            x,
            stop_id=tokenizer.eos_token_id,
            max_new_tokens=50,
            temperature=1.0,
            do_sample=False,
            num_beams=3
        )
        beam_response = tokenizer.decode(beam_output[0].tolist(), skip_special_tokens=True)

    print(f"æŸæœç´¢ç»“æœ: {beam_response}")
    print()

    # æµ‹è¯•4: ä¸åŒçš„æ¸©åº¦å‚æ•°å¯¹éšæœºé‡‡æ ·çš„å½±å“
    print("ğŸŒ¡ï¸ æµ‹è¯•4: ä¸åŒæ¸©åº¦å‚æ•°å¯¹éšæœºé‡‡æ ·çš„å½±å“")
    print("å‚æ•°: do_sample=True, num_beams=1, æµ‹è¯•ä¸åŒtemperatureå€¼")

    temperatures = [0.2, 0.8, 1.5]
    for temp in temperatures:
        with torch.no_grad():
            temp_output = model.generate_super(
                x,
                stop_id=tokenizer.eos_token_id,
                max_new_tokens=30,
                temperature=temp,
                do_sample=True,
                num_beams=1
            )
            temp_response = tokenizer.decode(temp_output[0].tolist(), skip_special_tokens=True)
            print(f"æ¸©åº¦ {temp}: {temp_response}")

    print()
    print("=" * 60)
    print("âœ… ä¸‰ç§è§£ç ç­–ç•¥æµ‹è¯•å®Œæˆï¼")
    print()
    print("ğŸ“Š æ€»ç»“å¯¹æ¯”:")
    print("â€¢ è´ªå©ªè§£ç : é€Ÿåº¦å¿«ï¼Œç»“æœç¡®å®šï¼Œé€‚åˆç¡®å®šæ€§ä»»åŠ¡")
    print("â€¢ éšæœºé‡‡æ ·: åˆ›é€ æ€§å¼ºï¼Œç»“æœå¤šæ ·ï¼Œé€‚åˆåˆ›æ„ç”Ÿæˆ")
    print("â€¢ æŸæœç´¢: è´¨é‡è¾ƒé«˜ï¼Œå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡ï¼Œé€‚åˆä¸€èˆ¬å¯¹è¯")

def test_original_generation():
    """
    åŸå§‹çš„ç”Ÿæˆä»£ç ä½œä¸ºå¯¹æ¯”
    """
    model_id = "../model/kmno4zx/happy-llm-215M-sft/"

    print("è¿è¡ŒåŸå§‹ç”Ÿæˆä»£ç ...")
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, device_map="cpu").eval()

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}
    ]

    input_ids = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)
    input_ids = tokenizer(input_ids).data['input_ids']

    x = (torch.tensor(input_ids, dtype=torch.long)[None, ...]).to(model.device)

    with torch.no_grad():
        y = model.generate_super(x, stop_id=tokenizer.eos_token_id, max_new_tokens=512, temperature=0.6)
        response = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)

    print(f"Assistant: {response}")

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•ä¸‰ç§è§£ç ç­–ç•¥...")
    print()

    try:
        test_decoding_strategies()
    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¿è¡ŒåŸå§‹ç”Ÿæˆä»£ç ...")
        test_original_generation()
