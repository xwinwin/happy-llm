# Extra-Chapterï¼štext-data-processing

æœ¬è¡¥å……ç« èŠ‚æ—¨åœ¨å¯¹å¤§æ¨¡å‹æ•°æ®å¤„ç†åšä¸€ä¸ªç®€å•çš„æ¢³ç†ä¸å…·ä½“çš„ä»£ç å®ç°ï¼Œå¸®åŠ©å¤§å®¶å¯¹äºå¤§æ¨¡å‹çš„æ•°æ®å¤„ç†æœ‰ä¸€ä¸ªæ›´åŠ æ¸…æ™°çš„è®¤è¯†ã€‚

## 1.ç†è§£è¯åµŒå…¥

åœ¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œç”±äºæ–‡æœ¬æ˜¯åˆ†ç±»æ•°æ®ï¼Œæ¨¡å‹æ— æ³•ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œå› æ­¤éœ€è¦å°†è¯è¯­è¡¨ç¤ºä¸ºè¿ç»­å€¼å‘é‡ä¼ è¾“ç»™æ¨¡å‹ã€‚è¿™ä¸€å¤„ç†è¿‡ç¨‹æˆ‘ä»¬ç§°ä¹‹ä¸ºè¯åµŒå…¥ï¼Œå…¶æœ¬è´¨æ˜¯å°†**ç¦»æ•£å¯¹è±¡**ï¼ˆå¦‚è¯è¯­ï¼‰æ˜ å°„åˆ°**è¿ç»­å‘é‡**ç©ºé—´ä¸­çš„ç‚¹ï¼Œç›®çš„æ˜¯å°†éæ•°å€¼æ•°æ®è½¬æ¢ä¸ºç¥ç»ç½‘ç»œå¯å¤„ç†çš„æ ¼å¼ã€‚

é€šè¿‡å°†æ¯ä¸ªè¯æ˜ å°„ä¸ºé«˜ç»´ç©ºé—´ä¸­çš„ç‚¹ï¼ˆå¦‚â€œçŒ«â€=[0.2, -1.7, ...]ï¼‰ï¼Œä½¿è¯­ä¹‰ç›¸ä¼¼çš„è¯ï¼ˆå¦‚â€œçŒ«â€å’Œâ€œç‹—â€ï¼‰å‘é‡è·ç¦»æ›´è¿‘ï¼Œè€Œæ— å…³è¯ï¼ˆå¦‚â€œçŒ«â€å’Œâ€œæ±½è½¦â€ï¼‰è·ç¦»æ›´è¿œã€‚è¿™ç§å‘é‡åŒ–è¡¨ç¤ºæ—¢ä¿ç•™è¯è¯­å…³ç³»ï¼ˆâ€œå›½ç‹-ç”·æ€§+å¥³æ€§â‰ˆå¥³ç‹â€ï¼‰ï¼Œåˆèƒ½ä½œä¸ºç¥ç»ç½‘ç»œçš„è¾“å…¥ï¼Œæ˜¯å¤§æ¨¡å‹ç†è§£è¯­è¨€çš„åŸºç¡€ã€‚

æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹ä¸‹é¢çš„ä»£ç ç¤ºä¾‹


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import re

# ç¤ºä¾‹æ–‡æœ¬ï¼ˆå¯è‡ªè¡Œä¿®æ”¹å¹¶ä¸”æµ‹è¯•ï¼‰
text = "Hello, how are you today? I hope you are doing well."

# ç®€å•åˆ†è¯å‡½æ•°
def tokenize(text):
    # æŒ‰éå­—æ¯æ•°å­—å­—ç¬¦åˆ†å‰²æ–‡æœ¬ï¼Œå¹¶ä¿ç•™åˆ†éš”ç¬¦
    tokens = re.findall(r'\w+|[^\w\s]', text)
    return tokens
# æ„å»ºè¯æ±‡è¡¨
def build_vocab(tokens):
    unique_tokens = sorted(set(tokens))
    vocab = {token: i for i, token in enumerate(unique_tokens)}
    vocab_size = len(vocab)
    return vocab, vocab_size
# æ–‡æœ¬ç¼–ç 
def encode_text(tokens, vocab):
    return [vocab.get(token, vocab['<unk>']) for token in tokens]

# åˆ›å»ºè¯åµŒå…¥æ¨¡å‹
class WordEmbeddingModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(WordEmbeddingModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
    def forward(self, inputs):
        return self.embedding(inputs)

# å¯è§†åŒ–è¯å‘é‡
def visualize_embeddings(embeddings, token_ids, vocab):
    reverse_vocab = {i: token for token, i in vocab.items()}
    pca = PCA(n_components=2)
    embeddings_2d = pca.fit_transform(embeddings.detach().numpy())
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    plt.figure(figsize=(10, 8))
    for i, token_id in enumerate(token_ids):
        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], marker='o')
        plt.annotate(reverse_vocab[token_id], 
                     (embeddings_2d[i, 0], embeddings_2d[i, 1]),
                     xytext=(5, 2), 
                     textcoords='offset points',
                     ha='right', 
                     va='bottom')
    plt.title('Word Embeddings Visualization')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.grid(True)
    plt.savefig('word_embeddings.png')
    plt.show()

def main():
    tokens = tokenize(text)
    print(f"Tokens: {tokens}")
    tokens_with_special = tokens + ['<pad>', '<unk>']
    
    # æ„å»ºè¯æ±‡è¡¨
    vocab, vocab_size = build_vocab(tokens_with_special)
    print(f"Vocabulary size: {vocab_size}")
    print(f"Vocabulary: {vocab}")
    
    # ç¼–ç æ–‡æœ¬
    encoded_text = encode_text(tokens, vocab)
    print(f"Encoded text: {encoded_text}")
    input_tensor = torch.tensor(encoded_text, dtype=torch.long)
    
    # åˆå§‹åŒ–æ¨¡å‹
    embedding_dim = 10  # è¯å‘é‡ç»´åº¦ï¼Œå³æåˆ°çš„è¯ç©ºé—´
    model = WordEmbeddingModel(vocab_size, embedding_dim)
    # è·å–è¯å‘é‡
    embeddings = model(input_tensor)
    print(f"Embeddings shape: {embeddings.shape}")  # è¿™é‡Œåº”è¯¥æ˜¯ [åºåˆ—é•¿åº¦, åµŒå…¥ç»´åº¦]
    
    print(f"Word vector for '{tokens[0]}': {embeddings[0].detach().numpy()}")
    print(f"Word vector for '{tokens[1]}': {embeddings[1].detach().numpy()}")
    # å¯è§†åŒ–è¯å‘é‡ï¼ˆä»…ç”¨äºä½ç»´æ¼”ç¤ºï¼Œå®é™…åº”ç”¨ä¸­è¯å‘é‡ç»´åº¦é€šå¸¸è¾ƒé«˜ï¼‰
    if embedding_dim >= 2:
        visualize_embeddings(embeddings, encoded_text, vocab)
    
    # è®¡ç®—è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦
    print("\nè¯ç›¸ä¼¼åº¦åˆ†æ:")
    for i, token_i in enumerate(tokens[:5]):  # åªåˆ†æå‰5ä¸ªè¯
        for j, token_j in enumerate(tokens[:5]):
            if i != j:
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                sim = F.cosine_similarity(embeddings[i].unsqueeze(0), 
                                         embeddings[j].unsqueeze(0))
                print(f" '{token_i}' å’Œ '{token_j}' çš„ç›¸ä¼¼åº¦: {sim.item():.4f}")

if __name__ == "__main__":
    main()    
```

    Tokens: ['Hello', ',', 'how', 'are', 'you', 'today', '?', 'I', 'hope', 'you', 'are', 'doing', 'well', '.']
    Vocabulary size: 14
    Vocabulary: {',': 0, '.': 1, '<pad>': 2, '<unk>': 3, '?': 4, 'Hello': 5, 'I': 6, 'are': 7, 'doing': 8, 'hope': 9, 'how': 10, 'today': 11, 'well': 12, 'you': 13}
    Encoded text: [5, 0, 10, 7, 13, 11, 4, 6, 9, 13, 7, 8, 12, 1]
    Embeddings shape: torch.Size([14, 10])
    Word vector for 'Hello': [-0.16787808 -0.46388683 -0.4728546   0.59449345 -0.23820949  0.34212282
      0.6591729  -0.10877569  0.60686487 -1.771871  ]
    Word vector for ',': [ 0.19194137 -1.2824519   1.1420391  -0.8361696  -0.578317    0.1025617
      1.2452478  -0.08552601  0.9869009  -0.04940421]
    


    
![png](./image/output_1_1.png)
    


    
    è¯ç›¸ä¼¼åº¦åˆ†æ:
     'Hello' å’Œ ',' çš„ç›¸ä¼¼åº¦: 0.2132
     'Hello' å’Œ 'how' çš„ç›¸ä¼¼åº¦: -0.0277
     'Hello' å’Œ 'are' çš„ç›¸ä¼¼åº¦: 0.1024
     'Hello' å’Œ 'you' çš„ç›¸ä¼¼åº¦: 0.1597
     ',' å’Œ 'Hello' çš„ç›¸ä¼¼åº¦: 0.2132
     ',' å’Œ 'how' çš„ç›¸ä¼¼åº¦: -0.0801
     ',' å’Œ 'are' çš„ç›¸ä¼¼åº¦: -0.6096
     ',' å’Œ 'you' çš„ç›¸ä¼¼åº¦: 0.2355
     'how' å’Œ 'Hello' çš„ç›¸ä¼¼åº¦: -0.0277
     'how' å’Œ ',' çš„ç›¸ä¼¼åº¦: -0.0801
     'how' å’Œ 'are' çš„ç›¸ä¼¼åº¦: 0.1170
     'how' å’Œ 'you' çš„ç›¸ä¼¼åº¦: -0.0412
     'are' å’Œ 'Hello' çš„ç›¸ä¼¼åº¦: 0.1024
     'are' å’Œ ',' çš„ç›¸ä¼¼åº¦: -0.6096
     'are' å’Œ 'how' çš„ç›¸ä¼¼åº¦: 0.1170
     'are' å’Œ 'you' çš„ç›¸ä¼¼åº¦: 0.2203
     'you' å’Œ 'Hello' çš„ç›¸ä¼¼åº¦: 0.1597
     'you' å’Œ ',' çš„ç›¸ä¼¼åº¦: 0.2355
     'you' å’Œ 'how' çš„ç›¸ä¼¼åº¦: -0.0412
     'you' å’Œ 'are' çš„ç›¸ä¼¼åº¦: 0.2203
    

è¿™ä¸‹æˆ‘ä»¬å°±èƒ½å¤Ÿå¾ˆæ¸…æ¥šçš„åœ¨æ•£ç‚¹å›¾ä¸­çœ‹åˆ°ä¸åŒè¯è¯­ä¹‹é—´çš„è·ç¦»ã€‚

## 2.åˆ†è¯

åˆ†è¯æ˜¯å°†è¾“å…¥æ–‡æœ¬æ‹†åˆ†ä¸ºå•ä¸ªæ ‡è®°ï¼ˆå¯ä»¥æ˜¯å•è¯ã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼‰çš„è¿‡ç¨‹ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå¥å­â€œHello, world!â€æ‹†æˆä¸€å°å—ä¸€å°å—çš„å½¢å¼ï¼Œå¦‚```["Hello", ",", "world", "!"]```ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿé€å—å¤„ç†è¯­è¨€ï¼Œè¿›ä¸€æ­¥è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å€¼å½¢å¼ã€‚

æˆ‘ä»¬å¯ä»¥ä»ç®€å•çš„åˆ†è¯æ–¹æ³•å…¥æ‰‹ï¼Œä¾‹å¦‚ä½¿ç”¨ Python çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆre åº“ï¼‰æŒ‰ç©ºç™½å­—ç¬¦æ‹†åˆ†æ–‡æœ¬ï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½å¯¼è‡´æ ‡ç‚¹ç¬¦å·ä¸å•è¯ç²˜è¿ï¼ˆå¦‚ â€œHello,â€ ä¸­çš„é€—å·æœªåˆ†ç¦»ï¼‰ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»£ç å½“ä¸­ä¿®æ”¹æ­£åˆ™è¡¨è¾¾å¼ï¼Œåœ¨ç©ºç™½å­—ç¬¦ã€é€—å·ã€å¥å·ç­‰å¤„è¿›è¡Œæ‹†åˆ†ï¼Œä½¿å•è¯ä¸æ ‡ç‚¹ç¬¦å·æˆä¸ºç‹¬ç«‹åˆ—è¡¨é¡¹ã€‚

åŒæ—¶ï¼Œåœ¨å¤„ç†çš„è¿‡ç¨‹ä¸­éœ€æ³¨æ„ä¿ç•™ç©ºç™½å­—ç¬¦æˆ–ç§»é™¤å†—ä½™ç©ºç™½ï¼Œä¾‹å¦‚å¤„ç† Python ä»£ç æ—¶éœ€ä¿ç•™ç¼©è¿›ï¼Œè€Œæ™®é€šæ–‡æœ¬å¯ç§»é™¤ã€‚
å€¼å¾—æ³¨æ„çš„æ˜¯ï¼šé€šå¸¸ä¸å°†æ–‡æœ¬è½¬ä¸ºå°å†™ï¼Œå› ä¸ºå¤§å°å†™æœ‰åŠ©äºæ¨¡å‹åŒºåˆ†ä¸“æœ‰åè¯ã€ç†è§£å¥å­ç»“æ„ã€‚
å®ç°ä»£ç å¦‚ä¸‹ï¼š


```python
import re
import tiktoken
from collections import defaultdict

class SimpleTokenizer:
    """ç®€å•åˆ†è¯å™¨å®ç°"""
    def __init__(self, text=None):
        self.vocab = {}
        self.reverse_vocab = {}
        self.vocab_size = 0
        self.special_tokens = {
            '<pad>': 0,
            '<unk>': 1,
            '<bos>': 2,
            '<eos>': 3
        }
        if text:
            self.build_vocab(text)

    def tokenize(self, text):
        """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ ‡è®°åˆ—è¡¨"""
        # ä¿ç•™ç©ºç™½å­—ç¬¦çš„åˆ†è¯
        tokens = re.findall(r'\S+|\s+', text)
        return tokens
    
    def build_vocab(self, text):
        """æ„å»ºè¯æ±‡è¡¨"""
        self.vocab = self.special_tokens.copy()
        self.vocab_size = len(self.special_tokens)
        tokens = self.tokenize(text)
        token_counts = defaultdict(int)
        for token in tokens:
            token_counts[token] += 1
        sorted_tokens = sorted(token_counts.items(), key=lambda x: (-x[1], x[0]))
        for token, _ in sorted_tokens:
            if token not in self.vocab:
                self.vocab[token] = self.vocab_size
                self.vocab_size += 1
        self.reverse_vocab = {v: k for k, v in self.vocab.items()}
    
    def encode(self, text):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°IDåºåˆ—"""
        tokens = self.tokenize(text)
        return [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]
    
    def decode(self, token_ids):
        """å°†æ ‡è®°IDåºåˆ—è½¬æ¢å›æ–‡æœ¬"""
        tokens = []
        for token_id in token_ids:
            if token_id in self.reverse_vocab:
                tokens.append(self.reverse_vocab[token_id])
            else:
                tokens.append('<unk>')
        text = ''
        prev_token = None
        for token in tokens:
            if prev_token and prev_token.isspace():
                text += token
            else:
                text = text.rstrip() + token
            prev_token = token
            
        return text


class BPETokenizer:
    """åŸºäºå­—èŠ‚å¯¹ç¼–ç (BPE)çš„åˆ†è¯å™¨"""
    def __init__(self, model_name="gpt2"):
        """
        åˆå§‹åŒ–BPEåˆ†è¯å™¨
        model_name: æ¨¡å‹åç§°ï¼Œå¦‚"gpt2"æˆ–"cl100k_base"(OpenAIçš„text-embedding-ada-002ä½¿ç”¨)
        """
        self.encoder = tiktoken.get_encoding(model_name)
        self.vocab_size = self.encoder.n_vocab
    
    def tokenize(self, text):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºBPEæ ‡è®°åˆ—è¡¨"""
        token_ids = self.encoder.encode(text)
        tokens = [self.encoder.decode_single_token_bytes(token_id) for token_id in token_ids]
        return tokens
    
    def encode(self, text):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºBPEæ ‡è®°IDåºåˆ—"""
        return self.encoder.encode(text)
    
    def decode(self, token_ids):
        """å°†BPEæ ‡è®°IDåºåˆ—è½¬æ¢å›æ–‡æœ¬"""
        return self.encoder.decode(token_ids)

if __name__ == "__main__":
    sample_text = "Hello, how are you today? I hope you are doing well."
    print("=== ç®€å•åˆ†è¯å™¨ ===")
    simple_tokenizer = SimpleTokenizer(sample_text)
    tokens = simple_tokenizer.tokenize(sample_text)
    print(f"åˆ†è¯ç»“æœ: {tokens}")
    encoded = simple_tokenizer.encode(sample_text)
    print(f"ç¼–ç ç»“æœ: {encoded}")
    decoded = simple_tokenizer.decode(encoded)
    print(f"è§£ç ç»“æœ: {decoded}")
    print(f"è¯æ±‡è¡¨å¤§å°: {simple_tokenizer.vocab_size}")
    print("\n=== BPEåˆ†è¯å™¨ (GPT-2) ===")
    bpe_tokenizer = BPETokenizer("gpt2")
    bpe_tokens = bpe_tokenizer.tokenize(sample_text)
    print(f"BPEåˆ†è¯ç»“æœ: {bpe_tokens}")
    bpe_encoded = bpe_tokenizer.encode(sample_text)
    print(f"BPEç¼–ç ç»“æœ: {bpe_encoded}")
    bpe_decoded = bpe_tokenizer.decode(bpe_encoded)
    print(f"BPEè§£ç ç»“æœ: {bpe_decoded}")
    print(f"BPEè¯æ±‡è¡¨å¤§å°: {bpe_tokenizer.vocab_size}")    
```

    === ç®€å•åˆ†è¯å™¨ ===
    åˆ†è¯ç»“æœ: ['Hello,', ' ', 'how', ' ', 'are', ' ', 'you', ' ', 'today?', ' ', 'I', ' ', 'hope', ' ', 'you', ' ', 'are', ' ', 'doing', ' ', 'well.']
    ç¼–ç ç»“æœ: [7, 4, 11, 4, 5, 4, 6, 4, 12, 4, 8, 4, 10, 4, 6, 4, 5, 4, 9, 4, 13]
    è§£ç ç»“æœ: Hello, how are you today? I hope you are doing well.
    è¯æ±‡è¡¨å¤§å°: 14
    
    === BPEåˆ†è¯å™¨ (GPT-2) ===
    BPEåˆ†è¯ç»“æœ: [b'Hello', b',', b' how', b' are', b' you', b' today', b'?', b' I', b' hope', b' you', b' are', b' doing', b' well', b'.']
    BPEç¼–ç ç»“æœ: [15496, 11, 703, 389, 345, 1909, 30, 314, 2911, 345, 389, 1804, 880, 13]
    BPEè§£ç ç»“æœ: Hello, how are you today? I hope you are doing well.
    BPEè¯æ±‡è¡¨å¤§å°: 50257
    

## 3.å°†æ ‡è®°è½¬æ¢ä¸ºæ ‡è®°ID

å°†æ ‡è®°è½¬æ¢ä¸ºæ ‡è®° ID æ˜¯è¿æ¥æ–‡æœ¬ä¸æ•°å€¼å‘é‡çš„ä¸­é—´æ­¥éª¤ï¼Œå…¶æ ¸å¿ƒæ˜¯æ„å»º â€œè¯æ±‡è¡¨â€â€”â€” ä¸€ä¸ªä»å”¯ä¸€æ ‡è®°åˆ°å”¯ä¸€æ•´æ•°çš„æ˜ å°„ï¼ˆå¦‚å›¾ 2.6 æ‰€ç¤ºï¼‰ã€‚è¯æ±‡è¡¨çš„æ„å»ºè¿‡ç¨‹æ˜¯ï¼šä»åˆ†è¯åçš„æ–‡æœ¬ä¸­æå–æ‰€æœ‰å”¯ä¸€æ ‡è®°ï¼ŒæŒ‰å­—æ¯é¡ºåºæ’åºåï¼Œä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…ä¸€ä¸ªæ•´æ•° IDã€‚ä¾‹å¦‚ï¼Œã€Šåˆ¤å†³ã€‹åˆ†è¯åå¾—åˆ° 1130 ä¸ªå”¯ä¸€æ ‡è®°ï¼Œè¯æ±‡è¡¨ä¾¿ä¼šå°†è¿™äº›æ ‡è®°åˆ†åˆ«æ˜ å°„åˆ° 0 è‡³ 1129 çš„æ•´æ•°ã€‚

æœ‰äº†è¯æ±‡è¡¨ï¼Œæˆ‘ä»¬å°±èƒ½é€šè¿‡ â€œç¼–ç â€ å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®° IDï¼ˆä¾‹å¦‚ â€œHelloâ€ å¯¹åº”æŸä¸ªæ•´æ•°ï¼‰ï¼Œä¹Ÿèƒ½é€šè¿‡ â€œè§£ç â€ å°†æ ‡è®° ID è½¬å›æ–‡æœ¬ã€‚è¿™ä¸€è¿‡ç¨‹å¯é€šè¿‡åˆ†è¯å™¨ç±»å®ç°ï¼Œä¾‹å¦‚ SimpleTokenizerV1 åŒ…å« encode å’Œ decode æ–¹æ³•ï¼šencode å…ˆå¯¹æ–‡æœ¬åˆ†è¯ï¼Œå†ç”¨è¯æ±‡è¡¨æ˜ å°„ä¸º IDï¼›decode åˆ™å°† ID é€šè¿‡åå‘æ˜ å°„è½¬å›æ–‡æœ¬ï¼Œå¹¶å¤„ç†æ ‡ç‚¹ç¬¦å·å‰çš„ç©ºæ ¼é—®é¢˜ã€‚

ä½†éœ€æ³¨æ„ï¼Œè‹¥æ–‡æœ¬ä¸­å‡ºç°è¯æ±‡è¡¨å¤–çš„æ ‡è®°ï¼ˆå¦‚ â€œHelloâ€ æœªå‡ºç°åœ¨ã€Šåˆ¤å†³ã€‹ä¸­ï¼‰ï¼Œç¼–ç æ—¶ä¼šæŠ¥é”™ï¼Œè¿™è¯´æ˜è®­ç»ƒé›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§å¯¹æ‰©å±•è¯æ±‡è¡¨è‡³å…³é‡è¦ï¼Œä¹Ÿå¼•å‡ºäº†åç»­å¤„ç†æœªçŸ¥è¯æ±‡çš„éœ€æ±‚ã€‚

æ•´ä½“å®ç°ä»£ç å¦‚ä¸‹ï¼š


```python
import re
from collections import Counter
from typing import List, Dict, Tuple, Optional, Set
class Tokenizer:
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°å¹¶æ˜ å°„åˆ°IDçš„åˆ†è¯å™¨"""
    
    def __init__(self, special_tokens: Optional[Dict[str, int]] = None):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨
        
        Args:
            special_tokens: ç‰¹æ®Šæ ‡è®°åŠå…¶IDçš„å­—å…¸ï¼Œé»˜è®¤ä¸ºNone
        """
        self.vocab: Dict[str, int] = {}  # è¯æ±‡è¡¨: æ ‡è®° -> ID
        self.reverse_vocab: Dict[int, str] = {}  # åå‘è¯æ±‡è¡¨: ID -> æ ‡è®°
        self.special_tokens = special_tokens or {
            "<pad>": 0,  # å¡«å……æ ‡è®°
            "<unk>": 1,  # æœªçŸ¥æ ‡è®°
            "<bos>": 2,  # åºåˆ—å¼€å§‹æ ‡è®°
            "<eos>": 3   # åºåˆ—ç»“æŸæ ‡è®°
        }
        self.vocab_size: int = len(self.special_tokens)  # è¯æ±‡è¡¨å¤§å°
        # åˆå§‹åŒ–è¯æ±‡è¡¨ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°
        for token, idx in self.special_tokens.items():
            self.vocab[token] = idx
            self.reverse_vocab[idx] = token
    
    def build_vocab(self, texts: List[str], min_freq: int = 1) -> None:
        """
        ä»æ–‡æœ¬æ„å»ºè¯æ±‡è¡¨
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            min_freq: æœ€å°è¯é¢‘ï¼Œä½äºæ­¤é¢‘ç‡çš„è¯å°†è¢«å¿½ç•¥
        """
        # ç»Ÿè®¡è¯é¢‘
        token_counts = Counter()
        for text in texts:
            tokens = self.tokenize(text)
            token_counts.update(tokens)
        # æŒ‰é¢‘ç‡æ’åºï¼Œé¢‘ç‡ç›¸åŒåˆ™æŒ‰å­—æ¯é¡ºåº
        sorted_tokens = sorted(
            [(token, count) for token, count in token_counts.items() if count >= min_freq],
            key=lambda x: (-x[1], x[0])  
        )
        # ä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…ID
        for token, _ in sorted_tokens:
            if token not in self.vocab:
                self.vocab[token] = self.vocab_size
                self.reverse_vocab[self.vocab_size] = token
                self.vocab_size += 1
    
    def tokenize(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ ‡è®°åˆ—è¡¨
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ ‡è®°åˆ—è¡¨
        """
        tokens = re.findall(r'\w+|[^\w\s]|\s+', text)
        return tokens
    
    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°IDåˆ—è¡¨
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            add_special_tokens: æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°
            
        Returns:
            æ ‡è®°IDåˆ—è¡¨
        """
        tokens = self.tokenize(text)
        encoded = [self.vocab.get(token, self.vocab["<unk>"]) for token in tokens]
        
        if add_special_tokens:
            encoded = [self.vocab["<bos>"]] + encoded + [self.vocab["<eos>"]]
            
        return encoded
    
    def decode(self, ids: List[int], remove_special_tokens: bool = False) -> str:
        """
        å°†æ ‡è®°IDåˆ—è¡¨è½¬æ¢å›æ–‡æœ¬
        Args:
            ids: æ ‡è®°IDåˆ—è¡¨
            remove_special_tokens: æ˜¯å¦ç§»é™¤ç‰¹æ®Šæ ‡è®°
        Returns:
            æ–‡æœ¬
        """
        tokens = []
        for idx in ids:
            if idx in self.reverse_vocab:
                token = self.reverse_vocab[idx]
                if remove_special_tokens and token in self.special_tokens:
                    continue
                tokens.append(token)
            else:
                tokens.append("<unk>")
        text = ''.join(tokens)
        return re.sub(r'\s+', ' ', text).strip()

def test_tokenizer():
    """æµ‹è¯•Tokenizerç±»çš„åŠŸèƒ½"""
    print("===== æµ‹è¯• Tokenizer =====")
    texts = [
        "Hello, how are you today?",
        "I hope you are doing well.",
        "This is a test of the tokenizer."
    ]

    tokenizer = Tokenizer()
    tokenizer.build_vocab(texts)
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"å‰10ä¸ªè¯æ±‡é¡¹: {list(tokenizer.vocab.items())[:10]}")
    sample_text = "Hello, this is a test!"
    tokens = tokenizer.tokenize(sample_text)
    print(f"\nåˆ†è¯ç»“æœ: {tokens}")
    encoded = tokenizer.encode(sample_text)
    print(f"ç¼–ç ç»“æœ: {encoded}")
    encoded_with_special = tokenizer.encode(sample_text, add_special_tokens=True)
    print(f"å¸¦ç‰¹æ®Šæ ‡è®°çš„ç¼–ç ç»“æœ: {encoded_with_special}")
    decoded = tokenizer.decode(encoded)
    print(f"è§£ç ç»“æœ: {decoded}")
    decoded_with_special = tokenizer.decode(encoded_with_special, remove_special_tokens=True)
    print(f"ç§»é™¤ç‰¹æ®Šæ ‡è®°çš„è§£ç ç»“æœ: {decoded_with_special}")
    unknown_text = "This is a unicorn ğŸ¦„ test."
    encoded_unknown = tokenizer.encode(unknown_text)
    decoded_unknown = tokenizer.decode(encoded_unknown)
    print(f"\nåŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: {unknown_text}")
    print(f"ç¼–ç ç»“æœ: {encoded_unknown}")
    print(f"è§£ç ç»“æœ: {decoded_unknown}")

if __name__ == "__main__":
    test_tokenizer()    
```

    ===== æµ‹è¯• Tokenizer =====
    è¯æ±‡è¡¨å¤§å°: 24
    å‰10ä¸ªè¯æ±‡é¡¹: [('<pad>', 0), ('<unk>', 1), ('<bos>', 2), ('<eos>', 3), (' ', 4), ('.', 5), ('are', 6), ('you', 7), (',', 8), ('?', 9)]
    
    åˆ†è¯ç»“æœ: ['Hello', ',', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test', '!']
    ç¼–ç ç»“æœ: [10, 8, 4, 1, 4, 17, 4, 13, 4, 19, 1]
    å¸¦ç‰¹æ®Šæ ‡è®°çš„ç¼–ç ç»“æœ: [2, 10, 8, 4, 1, 4, 17, 4, 13, 4, 19, 1, 3]
    è§£ç ç»“æœ: Hello, <unk> is a test<unk>
    ç§»é™¤ç‰¹æ®Šæ ‡è®°çš„è§£ç ç»“æœ: Hello, is a test
    
    åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: This is a unicorn ğŸ¦„ test.
    ç¼–ç ç»“æœ: [12, 4, 17, 4, 13, 4, 1, 4, 1, 4, 19, 5]
    è§£ç ç»“æœ: This is a <unk> <unk> test.
    

## 4.æ·»åŠ ç‰¹æ®Šä¸Šä¸‹æ–‡æ ‡è®°

ä¸ºè§£å†³æœªçŸ¥è¯æ±‡é—®é¢˜å¹¶å¢å¼ºæ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£ï¼Œéœ€å¼•å…¥ç‰¹æ®Šæ ‡è®°ã€‚å¸¸è§çš„ç‰¹æ®Šæ ‡è®°åŒ…æ‹¬ï¼š<|unk|>ï¼ˆè¡¨ç¤ºæœªçŸ¥è¯æ±‡ï¼‰ã€ï¼ˆåˆ†éš”ä¸åŒæ–‡æœ¬æ¥æºï¼‰ã€[BOS]ï¼ˆåºåˆ—å¼€å§‹ï¼‰ã€[EOS]ï¼ˆåºåˆ—ç»“æŸï¼‰ã€[PAD]ï¼ˆå¡«å……çŸ­æ–‡æœ¬è‡³ç»Ÿä¸€é•¿åº¦ï¼‰ç­‰ã€‚

ä¾‹å¦‚ï¼Œä¿®æ”¹è¯æ±‡è¡¨åŠ å…¥ <|unk|> å’Œåï¼Œåˆ†è¯å™¨ï¼ˆå¦‚ SimpleTokenizerV2ï¼‰åœ¨é‡åˆ°æœªçŸ¥è¯æ—¶ä¼šè‡ªåŠ¨æ›¿æ¢ä¸º <|unk|>ï¼Œå¹¶åœ¨ä¸åŒæ–‡æœ¬é—´æ’å…¥ä½œä¸ºåˆ†éš”ã€‚è¿™ä¸€è°ƒæ•´ä½¿æ¨¡å‹èƒ½å¤„ç†æœªè§è¿‡çš„è¯æ±‡ï¼Œå¹¶åŒºåˆ†ç‹¬ç«‹æ–‡æœ¬æ¥æºã€‚éœ€æ³¨æ„ï¼ŒGPT æ¨¡å‹é€šå¸¸ä»…ä½¿ç”¨ä½œä¸ºåˆ†éš”ç¬¦å’Œå¡«å……ç¬¦ï¼Œè€Œä¸ä¾èµ– <|unk|>ï¼Œå› ä¸ºå…¶é‡‡ç”¨çš„å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰åˆ†è¯å™¨èƒ½é€šè¿‡å­è¯åˆ†è§£å¤„ç†æœªçŸ¥è¯ï¼Œè¿™ä¹Ÿæ˜¯åç»­å°†ä»‹ç»çš„æ›´é«˜æ•ˆæ–¹æ³•ã€‚


```python
import re
from typing import List, Dict, Optional, Set

class Tokenizer:
    """å¸¦æœ‰ç‰¹æ®Šä¸Šä¸‹æ–‡æ ‡è®°çš„åˆ†è¯å™¨"""
    def __init__(self, special_tokens: Optional[Dict[str, int]] = None):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨
        
        Args:
            special_tokens: ç‰¹æ®Šæ ‡è®°åŠå…¶IDçš„å­—å…¸ï¼Œé»˜è®¤ä¸ºNone
        """
        # è®¾ç½®é»˜è®¤ç‰¹æ®Šæ ‡è®°
        self.default_special_tokens = {
            "<pad>": 0,  # å¡«å……æ ‡è®°
            "<unk>": 1,  # æœªçŸ¥æ ‡è®°
            "<bos>": 2,  # åºåˆ—å¼€å§‹æ ‡è®°
            "<eos>": 3,  # åºåˆ—ç»“æŸæ ‡è®°
            "<sep>": 4,  # åˆ†éš”æ ‡è®°
            "<cls>": 5,  # åˆ†ç±»æ ‡è®°
        }
        # åˆå¹¶ç”¨æˆ·æä¾›çš„ç‰¹æ®Šæ ‡è®°
        self.special_tokens = {**self.default_special_tokens, **(special_tokens or {})}
        # åˆå§‹åŒ–è¯æ±‡è¡¨
        self.vocab: Dict[str, int] = {}
        self.reverse_vocab: Dict[int, str] = {}
        self.vocab_size: int = len(self.special_tokens)
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°åˆ°è¯æ±‡è¡¨
        for token, idx in self.special_tokens.items():
            self.vocab[token] = idx
            self.reverse_vocab[idx] = token
    
    def build_vocab(self, texts: List[str], min_freq: int = 1) -> None:
        """
        ä»æ–‡æœ¬æ„å»ºè¯æ±‡è¡¨
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            min_freq: æœ€å°è¯é¢‘ï¼Œä½äºæ­¤é¢‘ç‡çš„è¯å°†è¢«å¿½ç•¥
        """
        # ç»Ÿè®¡è¯é¢‘
        token_counts = {}
        for text in texts:
            tokens = self.tokenize(text)
            for token in tokens:
                token_counts[token] = token_counts.get(token, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åºï¼Œé¢‘ç‡ç›¸åŒåˆ™æŒ‰å­—æ¯é¡ºåº
        sorted_tokens = sorted(
            [(token, count) for token, count in token_counts.items() if count >= min_freq],
            key=lambda x: (-x[1], x[0])  # æŒ‰é¢‘ç‡é™åºï¼Œå­—æ¯å‡åº
        )
        
        # ä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…ID
        for token, _ in sorted_tokens:
            if token not in self.vocab:
                self.vocab[token] = self.vocab_size
                self.reverse_vocab[self.vocab_size] = token
                self.vocab_size += 1
    
    def tokenize(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ ‡è®°åˆ—è¡¨
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ ‡è®°åˆ—è¡¨
        """
        # ç®€å•çš„åˆ†è¯ï¼šæŒ‰éå­—æ¯æ•°å­—å­—ç¬¦åˆ†å‰²ï¼Œä¿ç•™ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·
        tokens = re.findall(r'\w+|[^\w\s]|\s+', text)
        return tokens
    
    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°IDåˆ—è¡¨
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            add_special_tokens: æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°
            
        Returns:
            æ ‡è®°IDåˆ—è¡¨
        """
        tokens = self.tokenize(text)
        encoded = [self.vocab.get(token, self.vocab["<unk>"]) for token in tokens]
        if add_special_tokens:
            encoded = [self.vocab["<bos>"]] + encoded + [self.vocab["<eos>"]]
        return encoded
    def decode(self, ids: List[int], remove_special_tokens: bool = False) -> str:
        """
        å°†æ ‡è®°IDåˆ—è¡¨è½¬æ¢å›æ–‡æœ¬
        Args:
            ids: æ ‡è®°IDåˆ—è¡¨
            remove_special_tokens: æ˜¯å¦ç§»é™¤ç‰¹æ®Šæ ‡è®°
        Returns:
            æ–‡æœ¬
        """
        tokens = []
        for idx in ids:
            if idx in self.reverse_vocab:
                token = self.reverse_vocab[idx]
                if remove_special_tokens and token in self.special_tokens:
                    continue
                tokens.append(token)
            else:
                tokens.append("<unk>")
        text = ''.join(tokens)
        return re.sub(r'\s+', ' ', text).strip()
    
    def add_special_token(self, token: str) -> int:
        """
        æ·»åŠ æ–°çš„ç‰¹æ®Šæ ‡è®°
        Args:
            token: ç‰¹æ®Šæ ‡è®°å­—ç¬¦ä¸²
        Returns:
            æ–°æ ‡è®°çš„ID
        """
        if token in self.vocab:
            return self.vocab[token]
        new_id = self.vocab_size
        self.vocab[token] = new_id
        self.reverse_vocab[new_id] = token
        self.vocab_size += 1
        return new_id
    
def test_special_tokens():
    """æµ‹è¯•ç‰¹æ®Šæ ‡è®°çš„åŠŸèƒ½"""
    print("===== æµ‹è¯•ç‰¹æ®Šæ ‡è®° =====")
    texts = [
        "Hello, how are you today?",
        "I hope you are doing well.",
        "This is a test of the tokenizer."
    ]
    tokenizer = Tokenizer()
    tokenizer.build_vocab(texts)
    sample_text = "Hello, this is a test!"
    encoded = tokenizer.encode(sample_text, add_special_tokens=True)
    decoded = tokenizer.decode(encoded, remove_special_tokens=True)
    
    print(f"åŸå§‹æ–‡æœ¬: {sample_text}")
    print(f"ç¼–ç ç»“æœ: {encoded}")
    print(f"è§£ç ç»“æœ: {decoded}")
    print("\n--- æµ‹è¯•ç‰¹æ®Šæ ‡è®° ---")
    print(f"ç‰¹æ®Šæ ‡è®°: {tokenizer.special_tokens}")
    # æµ‹è¯•<bos>å’Œ<eos>
    encoded_with_bos_eos = tokenizer.encode(sample_text, add_special_tokens=True)
    print(f"å¸¦<bos>å’Œ<eos>çš„ç¼–ç : {encoded_with_bos_eos}")
    # æµ‹è¯•<SEP>æ ‡è®° - è¿æ¥ä¸¤ä¸ªå¥å­
    print("\n--- æµ‹è¯•<SEP>æ ‡è®° ---")
    sentence1 = "What is your name?"
    sentence2 = "My name is Doubao."
    encoded1 = tokenizer.encode(sentence1)
    encoded2 = tokenizer.encode(sentence2)
    # æ·»åŠ <SEP>æ ‡è®°
    encoded_combined = [tokenizer.vocab["<bos>"]] + \
                       encoded1 + \
                       [tokenizer.vocab["<sep>"]] + \
                       encoded2 + \
                       [tokenizer.vocab["<eos>"]]
    decoded_combined = tokenizer.decode(encoded_combined, remove_special_tokens=True)
    print(f"å¥å­1: {sentence1}")
    print(f"å¥å­2: {sentence2}")
    print(f"åˆå¹¶åçš„ç¼–ç : {encoded_combined}")
    print(f"åˆå¹¶åçš„è§£ç : {decoded_combined}")
    # æµ‹è¯•æ·»åŠ æ–°çš„ç‰¹æ®Šæ ‡è®°
    print("\n--- æµ‹è¯•æ·»åŠ æ–°çš„ç‰¹æ®Šæ ‡è®° ---")
    new_token = "<mask>"
    new_token_id = tokenizer.add_special_token(new_token)
    print(f"æ·»åŠ æ–°ç‰¹æ®Šæ ‡è®°: {new_token} (ID: {new_token_id})")
    
    # æµ‹è¯•ä½¿ç”¨æ–°çš„ç‰¹æ®Šæ ‡è®°
    masked_text = "This is a <mask> sentence."
    encoded_masked = tokenizer.encode(masked_text, add_special_tokens=True)
    decoded_masked = tokenizer.decode(encoded_masked, remove_special_tokens=False)
    
    print(f"å¸¦<mask>çš„æ–‡æœ¬: {masked_text}")
    print(f"ç¼–ç ç»“æœ: {encoded_masked}")
    print(f"è§£ç ç»“æœ: {decoded_masked}")


if __name__ == "__main__":
    test_special_tokens()    
```

    ===== æµ‹è¯•ç‰¹æ®Šæ ‡è®° =====
    åŸå§‹æ–‡æœ¬: Hello, this is a test!
    ç¼–ç ç»“æœ: [2, 12, 10, 6, 1, 6, 19, 6, 15, 6, 21, 1, 3]
    è§£ç ç»“æœ: Hello, is a test
    
    --- æµ‹è¯•ç‰¹æ®Šæ ‡è®° ---
    ç‰¹æ®Šæ ‡è®°: {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3, '<sep>': 4, '<cls>': 5}
    å¸¦<bos>å’Œ<eos>çš„ç¼–ç : [2, 12, 10, 6, 1, 6, 19, 6, 15, 6, 21, 1, 3]
    
    --- æµ‹è¯•<SEP>æ ‡è®° ---
    å¥å­1: What is your name?
    å¥å­2: My name is Doubao.
    åˆå¹¶åçš„ç¼–ç : [2, 1, 6, 19, 6, 1, 6, 1, 11, 4, 1, 6, 1, 6, 19, 6, 1, 7, 3]
    åˆå¹¶åçš„è§£ç : is ? is .
    
    --- æµ‹è¯•æ·»åŠ æ–°çš„ç‰¹æ®Šæ ‡è®° ---
    æ·»åŠ æ–°ç‰¹æ®Šæ ‡è®°: <mask> (ID: 26)
    å¸¦<mask>çš„æ–‡æœ¬: This is a <mask> sentence.
    ç¼–ç ç»“æœ: [2, 14, 6, 19, 6, 15, 6, 1, 1, 1, 6, 1, 7, 3]
    è§£ç ç»“æœ: <bos>This is a <unk><unk><unk> <unk>.<eos>
    

## 5.å­—èŠ‚å¯¹ç¼–ç 

å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰æ˜¯ä¸€ç§é«˜çº§åˆ†è¯æ–¹æ³•ï¼Œè¢« GPT-2ã€GPT-3 ç­‰ä¸»æµ LLMs é‡‡ç”¨ï¼Œå…¶æ ¸å¿ƒä¼˜åŠ¿æ˜¯èƒ½å¤„ç†æœªçŸ¥è¯æ±‡ â€”â€” å°†æœªè§è¿‡çš„å•è¯åˆ†è§£ä¸ºå­è¯å•å…ƒï¼ˆå¦‚å›¾ 2.11 æ‰€ç¤ºï¼‰ã€‚ä¾‹å¦‚ï¼Œâ€œsomeunknownPlaceâ€ å¯æ‹†åˆ†ä¸ºå·²çŸ¥çš„å­è¯æ ‡è®°ï¼Œæ— éœ€ä¾èµ– <|unk|>ã€‚

BPE çš„å®ç°å¯å€ŸåŠ© tiktoken åº“ï¼ˆOpenAI å¼€æºï¼‰ï¼Œå…¶åŸºäº Rust å®ç°ï¼Œé«˜æ•ˆä¸”å…¼å®¹ GPT æ¨¡å‹çš„åˆ†è¯é€»è¾‘ã€‚ä½¿ç”¨æ—¶ï¼Œå…ˆé€šè¿‡ tiktoken.get_encoding ("gpt2") å®ä¾‹åŒ–åˆ†è¯å™¨ï¼Œå†ç”¨ encode æ–¹æ³•å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®° IDï¼Œdecode æ–¹æ³•åˆ™å¯è¿˜åŸæ–‡æœ¬ã€‚ä¾‹å¦‚ï¼Œâ€œHello, do you like tea?â€ ç» BPE ç¼–ç åï¼Œä¼šç”Ÿæˆä¸€ç³»åˆ—æ•´æ•° IDï¼Œè§£ç åèƒ½å‡†ç¡®è¿˜åŸåŸå§‹æ–‡æœ¬ï¼Œå³ä½¿åŒ…å« â€œsomeunknownPlaceâ€ è¿™ç±»æœªçŸ¥è¯ï¼Œä¹Ÿèƒ½é€šè¿‡å­è¯æ‹†åˆ†æ­£ç¡®å¤„ç†ã€‚

BPE çš„è¯æ±‡è¡¨è§„æ¨¡å›ºå®šï¼ˆå¦‚ GPT-2 ä¸º 50257ï¼‰ï¼Œé€šè¿‡è¿­ä»£åˆå¹¶é«˜é¢‘å­—ç¬¦æˆ–å­è¯æ„å»ºï¼Œæ—¢æ§åˆ¶äº†è¯æ±‡è¡¨å¤§å°ï¼Œåˆèƒ½è¦†ç›–å‡ ä¹æ‰€æœ‰å¯èƒ½çš„æ–‡æœ¬ï¼Œæ˜¯å¹³è¡¡æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›çš„ç†æƒ³é€‰æ‹©ã€‚


```python
import re
import tiktoken
from collections import defaultdict, Counter
from typing import List, Dict, Tuple, Optional, Set, Union

class SimpleBPETokenizer:
    """ç®€å•å®ç°çš„BPEåˆ†è¯å™¨"""
    
    def __init__(self, vocab_size: int = 100):
        """
        åˆå§‹åŒ–BPEåˆ†è¯å™¨
        
        Args:
            vocab_size: ç›®æ ‡è¯æ±‡è¡¨å¤§å°
        """
        self.vocab_size = vocab_size
        self.vocab = {}  # è¯æ±‡è¡¨: æ ‡è®° -> ID
        self.reverse_vocab = {}  # åå‘è¯æ±‡è¡¨: ID -> æ ‡è®°
        self.bpe_ranks = {}  # BPEåˆå¹¶è§„åˆ™çš„ä¼˜å…ˆçº§
        self.special_tokens = {
            "<pad>": 0,
            "<unk>": 1,
            "<bos>": 2,
            "<eos>": 3
        }
        self.vocab_size_actual = len(self.special_tokens)
        
        # åˆå§‹åŒ–è¯æ±‡è¡¨ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°
        for token, idx in self.special_tokens.items():
            self.vocab[token] = idx
            self.reverse_vocab[idx] = token
    
    def _get_stats(self, pairs: Dict[Tuple[str, str], int]) -> Dict[Tuple[str, str], int]:
        """è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡"""
        stats = defaultdict(int)
        for word, freq in pairs.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                stats[symbols[i], symbols[i + 1]] += freq
        return stats
    
    def _merge_vocab(self, pair: Tuple[str, str], v_in: Dict[str, int]) -> Dict[str, int]:
        """åˆå¹¶æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹"""
        v_out = {}
        bigram = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        for word in v_in:
            w_out = p.sub(''.join(pair), word)
            v_out[w_out] = v_in[word]
        return v_out
    
    def _word_to_pairs(self, word: str) -> Set[Tuple[str, str]]:
        """å°†å•è¯æ‹†åˆ†ä¸ºç›¸é‚»å­—èŠ‚å¯¹"""
        symbols = word.split()
        pairs = set()
        if len(symbols) < 2:
            return pairs
        for i in range(len(symbols) - 1):
            pairs.add((symbols[i], symbols[i + 1]))
        return pairs
    
    def build_vocab(self, texts: List[str]) -> None:
        """
        ä»æ–‡æœ¬æ„å»ºBPEè¯æ±‡è¡¨
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
        """
        # åˆå§‹åŒ–è¯æ±‡è¡¨ï¼ŒåŒ…å«æ‰€æœ‰å•ä¸ªå­—ç¬¦
        token_counts = Counter()
        for text in texts:
            # å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå­—ç¬¦ï¼Œç”¨ç©ºæ ¼åˆ†éš”
            words = [' '.join(list(text))]
            for word in words:
                token_counts[word] += 1
        
        # ç»Ÿè®¡åˆå§‹çš„å­—ç¬¦è¯æ±‡è¡¨
        chars = set()
        for text in texts:
            chars.update(text)
        
        # æ·»åŠ å­—ç¬¦åˆ°è¯æ±‡è¡¨
        for char in sorted(chars):
            if char not in self.vocab:
                self.vocab[char] = self.vocab_size_actual
                self.reverse_vocab[self.vocab_size_actual] = char
                self.vocab_size_actual += 1
        
        # å¼€å§‹BPEåˆå¹¶è¿‡ç¨‹
        num_merges = self.vocab_size - self.vocab_size_actual
        if num_merges <= 0:
            return
        
        pairs = token_counts.copy()
        for i in range(num_merges):
            # è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡
            stats = self._get_stats(pairs)
            if not stats:
                break
            
            # é€‰æ‹©æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹
            best = max(stats, key=stats.get)
            
            # è®°å½•åˆå¹¶è§„åˆ™çš„ä¼˜å…ˆçº§
            self.bpe_ranks[best] = i
            
            # åˆå¹¶è¯æ±‡è¡¨ä¸­çš„å­—èŠ‚å¯¹
            pairs = self._merge_vocab(best, pairs)
            
            # å°†æ–°åˆå¹¶çš„æ ‡è®°æ·»åŠ åˆ°è¯æ±‡è¡¨
            new_token = ''.join(best)
            if new_token not in self.vocab:
                self.vocab[new_token] = self.vocab_size_actual
                self.reverse_vocab[self.vocab_size_actual] = new_token
                self.vocab_size_actual += 1
    
    def _get_pairs(self, word: List[str]) -> Set[Tuple[str, str]]:
        """è·å–å•è¯ä¸­æ‰€æœ‰ç›¸é‚»æ ‡è®°å¯¹"""
        pairs = set()
        prev_char = word[0]
        for char in word[1:]:
            pairs.add((prev_char, char))
            prev_char = char
        return pairs
    
    def bpe(self, token: str) -> List[str]:
        """
        å¯¹å•ä¸ªæ ‡è®°åº”ç”¨BPEç®—æ³•
        
        Args:
            token: è¾“å…¥æ ‡è®°
            
        Returns:
            BPEåˆ†è¯åçš„æ ‡è®°åˆ—è¡¨
        """
        if token in self.special_tokens:
            return [token]
        
        word = list(token)
        if len(word) == 0:
            return []
        if len(word) == 1:
            return [word[0]]
        
        pairs = self._get_pairs(word)
        
        while True:
            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜çš„å­—èŠ‚å¯¹
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
            if bigram not in self.bpe_ranks:
                break
            
            first, second = bigram
            new_word = []
            i = 0
            while i < len(word):
                try:
                    j = word.index(first, i)
                    new_word.extend(word[i:j])
                    i = j
                except:
                    new_word.extend(word[i:])
                    break
                
                if i < len(word) - 1 and word[i + 1] == second:
                    new_word.append(first + second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = self._get_pairs(word)
        
        return word
    
    def tokenize(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†è¯ä¸ºBPEæ ‡è®°
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            BPEæ ‡è®°åˆ—è¡¨
        """
        tokens = []
        for token in text.split():
            if token in self.special_tokens:
                tokens.append(token)
            else:
                tokens.extend(self.bpe(token))
        return tokens
    
    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºBPEæ ‡è®°IDåˆ—è¡¨
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            add_special_tokens: æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°
            
        Returns:
            BPEæ ‡è®°IDåˆ—è¡¨
        """
        tokens = self.tokenize(text)
        encoded = [self.vocab.get(token, self.vocab["<unk>"]) for token in tokens]
        
        if add_special_tokens:
            encoded = [self.vocab["<bos>"]] + encoded + [self.vocab["<eos>"]]
            
        return encoded
    
    def decode(self, ids: List[int], remove_special_tokens: bool = False) -> str:
        """
        å°†BPEæ ‡è®°IDåˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬
        
        Args:
            ids: BPEæ ‡è®°IDåˆ—è¡¨
            remove_special_tokens: æ˜¯å¦ç§»é™¤ç‰¹æ®Šæ ‡è®°
            
        Returns:
            è§£ç åçš„æ–‡æœ¬
        """
        tokens = []
        for idx in ids:
            if idx in self.reverse_vocab:
                token = self.reverse_vocab[idx]
                if remove_special_tokens and token in self.special_tokens:
                    continue
                tokens.append(token)
            else:
                tokens.append("<unk>")
        
        # ç®€å•çš„åå¤„ç†ï¼šåˆå¹¶æ ‡è®°
        text = ''.join(tokens)
        return text


# æµ‹è¯•ä»£ç 
def test_simple_bpe_tokenizer():
    """æµ‹è¯•ç®€å•å®ç°çš„BPEåˆ†è¯å™¨"""
    print("===== æµ‹è¯•ç®€å•BPEåˆ†è¯å™¨ =====")
    
    # ç¤ºä¾‹æ–‡æœ¬
    texts = [
        "Hello, how are you today?",
        "I hope you are doing well.",
        "This is a test of the BPE tokenizer."
    ]
    
    # åˆå§‹åŒ–BPEåˆ†è¯å™¨
    tokenizer = SimpleBPETokenizer(vocab_size=50)
    
    # æ„å»ºè¯æ±‡è¡¨
    tokenizer.build_vocab(texts)
    
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size_actual}")
    print(f"å‰10ä¸ªè¯æ±‡é¡¹: {list(tokenizer.vocab.items())[:10]}")
    
    # æµ‹è¯•åˆ†è¯
    sample_text = "Hello, this is a test!"
    tokens = tokenizer.tokenize(sample_text)
    print(f"\nåˆ†è¯ç»“æœ: {tokens}")
    
    # æµ‹è¯•ç¼–ç 
    encoded = tokenizer.encode(sample_text, add_special_tokens=True)
    print(f"ç¼–ç ç»“æœ: {encoded}")
    
    # æµ‹è¯•è§£ç 
    decoded = tokenizer.decode(encoded, remove_special_tokens=True)
    print(f"è§£ç ç»“æœ: {decoded}")
    
    # æµ‹è¯•åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬
    unknown_text = "This is a unicorn ğŸ¦„ test."
    encoded_unknown = tokenizer.encode(unknown_text)
    decoded_unknown = tokenizer.decode(encoded_unknown)
    print(f"\nåŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: {unknown_text}")
    print(f"ç¼–ç ç»“æœ: {encoded_unknown}")
    print(f"è§£ç ç»“æœ: {decoded_unknown}")
    
    print("\næ‰€æœ‰æµ‹è¯•å®Œæˆ!")


def test_tiktoken_bpe():
    """æµ‹è¯•tiktokenåº“çš„BPEåˆ†è¯å™¨"""
    print("\n===== æµ‹è¯•tiktoken BPEåˆ†è¯å™¨ =====")
    
    # åˆå§‹åŒ–tiktoken BPEåˆ†è¯å™¨
    try:
        tokenizer = tiktoken.get_encoding("gpt2")
    except KeyError:
        # å¦‚æœæ²¡æœ‰å®‰è£…gpt2ç¼–ç ï¼Œå°è¯•ä½¿ç”¨cl100k_base (ç”¨äºtext-embedding-ada-002)
        tokenizer = tiktoken.get_encoding("cl100k_base")
    
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.n_vocab}")
    
    # æµ‹è¯•åˆ†è¯
    sample_text = "Hello, this is a test!"
    tokens = tokenizer.encode(sample_text)
    print(f"\nç¼–ç ç»“æœ (ID): {tokens}")
    
    # è½¬æ¢IDä¸ºå­—èŠ‚
    token_bytes = [tokenizer.decode_single_token_bytes(token) for token in tokens]
    print(f"ç¼–ç ç»“æœ (å­—èŠ‚): {token_bytes}")
    
    # æµ‹è¯•è§£ç 
    decoded = tokenizer.decode(tokens)
    print(f"è§£ç ç»“æœ: {decoded}")
    
    # æµ‹è¯•åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬
    unknown_text = "This is a unicorn ğŸ¦„ test."
    encoded_unknown = tokenizer.encode(unknown_text)
    decoded_unknown = tokenizer.decode(encoded_unknown)
    print(f"\nåŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: {unknown_text}")
    print(f"ç¼–ç ç»“æœ: {encoded_unknown}")
    print(f"è§£ç ç»“æœ: {decoded_unknown}")
    
    # è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡
    print(f"\næ–‡æœ¬ '{sample_text}' çš„tokenæ•°é‡: {len(tokens)}")

if __name__ == "__main__":
    test_simple_bpe_tokenizer()
    test_tiktoken_bpe()    
```

    ===== æµ‹è¯•ç®€å•BPEåˆ†è¯å™¨ =====
    è¯æ±‡è¡¨å¤§å°: 46
    å‰10ä¸ªè¯æ±‡é¡¹: [('<pad>', 0), ('<unk>', 1), ('<bos>', 2), ('<eos>', 3), (' ', 4), (',', 5), ('.', 6), ('?', 7), ('B', 8), ('E', 9)]
    
    åˆ†è¯ç»“æœ: ['Hello,', 't', 'h', 'is', 'is', 'a', 't', 'e', 's', 't', '!']
    ç¼–ç ç»“æœ: [2, 44, 28, 19, 41, 41, 14, 28, 16, 27, 28, 1, 3]
    è§£ç ç»“æœ: Hello,thisisatest
    
    åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: This is a unicorn ğŸ¦„ test.
    ç¼–ç ç»“æœ: [13, 19, 41, 41, 14, 29, 23, 20, 1, 24, 26, 23, 1, 28, 16, 27, 28, 6]
    è§£ç ç»“æœ: Thisisauni<unk>orn<unk>test.
    
    æ‰€æœ‰æµ‹è¯•å®Œæˆ!
    
    ===== æµ‹è¯•tiktoken BPEåˆ†è¯å™¨ =====
    è¯æ±‡è¡¨å¤§å°: 50257
    
    ç¼–ç ç»“æœ (ID): [15496, 11, 428, 318, 257, 1332, 0]
    ç¼–ç ç»“æœ (å­—èŠ‚): [b'Hello', b',', b' this', b' is', b' a', b' test', b'!']
    è§£ç ç»“æœ: Hello, this is a test!
    
    åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: This is a unicorn ğŸ¦„ test.
    ç¼–ç ç»“æœ: [1212, 318, 257, 44986, 12520, 99, 226, 1332, 13]
    è§£ç ç»“æœ: This is a unicorn ğŸ¦„ test.
    
    æ–‡æœ¬ 'Hello, this is a test!' çš„tokenæ•°é‡: 7
    

## 6.ä½¿ç”¨æ»‘åŠ¨çª—å£è¿›è¡Œæ•°æ®é‡‡æ ·

LLMs é€šè¿‡ â€œä¸‹ä¸€ä¸ªè¯é¢„æµ‹â€ ä»»åŠ¡é¢„è®­ç»ƒï¼Œå³ç»™å®šè¾“å…¥æ–‡æœ¬å—ï¼Œé¢„æµ‹ç´§éšå…¶åçš„ä¸‹ä¸€ä¸ªè¯ï¼ˆå¦‚å›¾ 2.12 æ‰€ç¤ºï¼‰ã€‚ä¸ºç”Ÿæˆè®­ç»ƒæ‰€éœ€çš„è¾“å…¥ - ç›®æ ‡å¯¹ï¼Œéœ€é‡‡ç”¨ â€œæ»‘åŠ¨çª—å£â€ æ–¹æ³•ï¼šåœ¨åˆ†è¯åçš„æ–‡æœ¬ä¸Šæ»‘åŠ¨å›ºå®šå¤§å°çš„çª—å£ï¼Œçª—å£å†…çš„æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œçª—å£å³ä¾§ç´§é‚»çš„è¯ä½œä¸ºç›®æ ‡ï¼ˆè¾“å…¥å³ç§»ä¸€ä½å³ä¸ºç›®æ ‡ï¼‰ã€‚

ä¾‹å¦‚ï¼Œè‹¥çª—å£å¤§å°ï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰ä¸º 4ï¼Œè¾“å…¥ä¸º [æ ‡è®° 1, æ ‡è®° 2, æ ‡è®° 3, æ ‡è®° 4]ï¼Œåˆ™ç›®æ ‡ä¸º [æ ‡è®° 2, æ ‡è®° 3, æ ‡è®° 4, æ ‡è®° 5]ã€‚ä¸ºé«˜æ•ˆç”Ÿæˆæ‰¹é‡æ ·æœ¬ï¼Œå¯ç»“åˆ PyTorch çš„ Dataset å’Œ DataLoaderï¼šDataset è´Ÿè´£æŒ‰çª—å£æ»‘åŠ¨ç”Ÿæˆè¾“å…¥ - ç›®æ ‡å¯¹ï¼ŒDataLoader åˆ™å°†è¿™äº›å¯¹æ‰“åŒ…ä¸ºæ‰¹æ¬¡ï¼ˆå¦‚å›¾ 2.13 æ‰€ç¤ºï¼‰ï¼Œæ”¯æŒè®¾ç½®æ‰¹æ¬¡å¤§å°ã€æ­¥é•¿ï¼ˆçª—å£ç§»åŠ¨è·ç¦»ï¼‰ç­‰å‚æ•°ã€‚

æ­¥é•¿å†³å®šæ ·æœ¬é‡å ç¨‹åº¦ï¼šæ­¥é•¿ä¸º 1 æ—¶ï¼Œç›¸é‚»æ ·æœ¬é«˜åº¦é‡å ï¼›æ­¥é•¿ç­‰äºçª—å£å¤§å°æ—¶ï¼Œæ ·æœ¬æ— é‡å ã€‚åˆç†è®¾ç½®æ­¥é•¿å¯å¹³è¡¡æ•°æ®åˆ©ç”¨ç‡ä¸è¿‡æ‹Ÿåˆé£é™©ï¼Œæœ€ç»ˆç”Ÿæˆçš„è¾“å…¥ - ç›®æ ‡å¯¹ä»¥å¼ é‡å½¢å¼è¾“å…¥æ¨¡å‹ï¼Œä¸ºè®­ç»ƒæä¾›æ•°æ®æ”¯æŒã€‚



```python
import torch
from torch.utils.data import Dataset, DataLoader
from typing import List, Tuple, Optional

class TextWindowDataset(Dataset):
    """ä½¿ç”¨æ»‘åŠ¨çª—å£å¯¹æ–‡æœ¬è¿›è¡Œé‡‡æ ·çš„æ•°æ®é›†"""
    
    def __init__(self, 
                 text: List[int], 
                 context_length: int, 
                 stride: int = 1,
                 pad_id: int = 0):
        """
        åˆå§‹åŒ–æ–‡æœ¬çª—å£æ•°æ®é›†
        
        Args:
            text: å·²ç¼–ç çš„æ–‡æœ¬ï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰
            context_length: ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆçª—å£å¤§å°ï¼‰
            stride: æ»‘åŠ¨çª—å£çš„æ­¥é•¿ï¼Œé»˜è®¤ä¸º1
            pad_id: å¡«å……æ ‡è®°çš„ID
        """
        self.text = text
        self.context_length = context_length
        self.stride = stride
        self.pad_id = pad_id
        
        # è®¡ç®—æœ‰æ•ˆæ ·æœ¬æ•°é‡
        self.num_samples = max(0, (len(text) - context_length) // stride + 1)
        
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†çš„æ ·æœ¬æ•°é‡"""
        return self.num_samples
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        Args:
            idx: æ ·æœ¬ç´¢å¼•
            
        Returns:
            å…ƒç»„(inputs, targets)ï¼Œå…¶ä¸­inputsæ˜¯è¾“å…¥åºåˆ—ï¼Œtargetsæ˜¯ç›®æ ‡åºåˆ—
        """
        # è®¡ç®—çª—å£èµ·å§‹ä½ç½®
        start = idx * self.stride
        
        # ç¡®ä¿çª—å£ä¸è¶…å‡ºæ–‡æœ¬é•¿åº¦
        end = start + self.context_length
        if end > len(self.text):
            # æˆªå–æœ€åå¯èƒ½çš„æœ‰æ•ˆçª—å£
            end = len(self.text)
            start = end - self.context_length
        
        # æå–è¾“å…¥åºåˆ—å’Œç›®æ ‡åºåˆ—
        inputs = self.text[start:end]
        targets = self.text[start+1:end+1]  # ç›®æ ‡æ˜¯è¾“å…¥çš„ä¸‹ä¸€ä¸ªæ ‡è®°
        
        # å¦‚æœç›®æ ‡åºåˆ—é•¿åº¦ä¸è¶³ï¼Œç”¨pad_idå¡«å……
        if len(targets) < self.context_length:
            targets = targets + [self.pad_id] * (self.context_length - len(targets))
        
        # è½¬æ¢ä¸ºå¼ é‡
        inputs = torch.tensor(inputs, dtype=torch.long)
        targets = torch.tensor(targets, dtype=torch.long)
        
        return inputs, targets


def create_data_loader(text: List[int], 
                       context_length: int, 
                       batch_size: int, 
                       stride: int = 1,
                       shuffle: bool = False) -> DataLoader:
    """
    åˆ›å»ºæ–‡æœ¬çª—å£æ•°æ®åŠ è½½å™¨
    
    Args:
        text: å·²ç¼–ç çš„æ–‡æœ¬ï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰
        context_length: ä¸Šä¸‹æ–‡é•¿åº¦
        batch_size: æ‰¹æ¬¡å¤§å°
        stride: æ»‘åŠ¨çª—å£æ­¥é•¿
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
        
    Returns:
        æ•°æ®åŠ è½½å™¨
    """
    dataset = TextWindowDataset(
        text=text,
        context_length=context_length,
        stride=stride
    )
    
    data_loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle
    )
    
    return data_loader


# æµ‹è¯•ä»£ç 
def test_sliding_window():
    """æµ‹è¯•æ»‘åŠ¨çª—å£æ•°æ®é‡‡æ ·"""
    print("===== æµ‹è¯•æ»‘åŠ¨çª—å£æ•°æ®é‡‡æ · =====")
    
    # ç¤ºä¾‹æ–‡æœ¬ï¼ˆå·²ç¼–ç ï¼‰
    encoded_text = [101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
    print(f"åŸå§‹æ–‡æœ¬: {encoded_text}")
    
    # å‚æ•°è®¾ç½®
    context_length = 4
    stride = 2
    batch_size = 2
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = TextWindowDataset(
        text=encoded_text,
        context_length=context_length,
        stride=stride
    )
    
    # æ‰“å°æ•°æ®é›†ä¿¡æ¯
    print(f"\næ•°æ®é›†å¤§å°: {len(dataset)}")
    
    # æµ‹è¯•è·å–å•ä¸ªæ ·æœ¬
    print("\n--- æµ‹è¯•è·å–å•ä¸ªæ ·æœ¬ ---")
    for i in range(min(3, len(dataset))):
        inputs, targets = dataset[i]
        print(f"æ ·æœ¬ {i}:")
        print(f"  è¾“å…¥: {inputs}")
        print(f"  ç›®æ ‡: {targets}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    data_loader = create_data_loader(
        text=encoded_text,
        context_length=context_length,
        batch_size=batch_size,
        stride=stride,
        shuffle=False
    )
    
    # æµ‹è¯•æ‰¹æ¬¡æ•°æ®
    print("\n--- æµ‹è¯•æ‰¹æ¬¡æ•°æ® ---")
    for i, (batch_inputs, batch_targets) in enumerate(data_loader):
        print(f"æ‰¹æ¬¡ {i}:")
        print(f"  è¾“å…¥å½¢çŠ¶: {batch_inputs.shape}")
        print(f"  è¾“å…¥æ•°æ®:")
        print(batch_inputs)
        print(f"  ç›®æ ‡å½¢çŠ¶: {batch_targets.shape}")
        print(f"  ç›®æ ‡æ•°æ®:")
        print(batch_targets)
    
    # æµ‹è¯•ä¸åŒæ­¥é•¿
    print("\n--- æµ‹è¯•ä¸åŒæ­¥é•¿ ---")
    for stride in [1, 2, 3]:
        dataset = TextWindowDataset(
            text=encoded_text,
            context_length=context_length,
            stride=stride
        )
        print(f"æ­¥é•¿ä¸º {stride} æ—¶çš„æ ·æœ¬æ•°: {len(dataset)}")
        
        # æ‰“å°å‰ä¸¤ä¸ªæ ·æœ¬
        if len(dataset) > 0:
            inputs, targets = dataset[0]
            print(f"  ç¬¬ä¸€ä¸ªæ ·æœ¬è¾“å…¥: {inputs}")
            print(f"  ç¬¬ä¸€ä¸ªæ ·æœ¬ç›®æ ‡: {targets}")
        
        if len(dataset) > 1:
            inputs, targets = dataset[1]
            print(f"  ç¬¬äºŒä¸ªæ ·æœ¬è¾“å…¥: {inputs}")
            print(f"  ç¬¬äºŒä¸ªæ ·æœ¬ç›®æ ‡: {targets}")
if __name__ == "__main__":
    test_sliding_window()
```

    ===== æµ‹è¯•æ»‘åŠ¨çª—å£æ•°æ®é‡‡æ · =====
    åŸå§‹æ–‡æœ¬: [101, 102, 103, 104, 105, 106, 107, 108, 109, 110]
    
    æ•°æ®é›†å¤§å°: 4
    
    --- æµ‹è¯•è·å–å•ä¸ªæ ·æœ¬ ---
    æ ·æœ¬ 0:
      è¾“å…¥: tensor([101, 102, 103, 104])
      ç›®æ ‡: tensor([102, 103, 104, 105])
    æ ·æœ¬ 1:
      è¾“å…¥: tensor([103, 104, 105, 106])
      ç›®æ ‡: tensor([104, 105, 106, 107])
    æ ·æœ¬ 2:
      è¾“å…¥: tensor([105, 106, 107, 108])
      ç›®æ ‡: tensor([106, 107, 108, 109])
    
    --- æµ‹è¯•æ‰¹æ¬¡æ•°æ® ---
    æ‰¹æ¬¡ 0:
      è¾“å…¥å½¢çŠ¶: torch.Size([2, 4])
      è¾“å…¥æ•°æ®:
    tensor([[101, 102, 103, 104],
            [103, 104, 105, 106]])
      ç›®æ ‡å½¢çŠ¶: torch.Size([2, 4])
      ç›®æ ‡æ•°æ®:
    tensor([[102, 103, 104, 105],
            [104, 105, 106, 107]])
    æ‰¹æ¬¡ 1:
      è¾“å…¥å½¢çŠ¶: torch.Size([2, 4])
      è¾“å…¥æ•°æ®:
    tensor([[105, 106, 107, 108],
            [107, 108, 109, 110]])
      ç›®æ ‡å½¢çŠ¶: torch.Size([2, 4])
      ç›®æ ‡æ•°æ®:
    tensor([[106, 107, 108, 109],
            [108, 109, 110,   0]])
    
    --- æµ‹è¯•ä¸åŒæ­¥é•¿ ---
    æ­¥é•¿ä¸º 1 æ—¶çš„æ ·æœ¬æ•°: 7
      ç¬¬ä¸€ä¸ªæ ·æœ¬è¾“å…¥: tensor([101, 102, 103, 104])
      ç¬¬ä¸€ä¸ªæ ·æœ¬ç›®æ ‡: tensor([102, 103, 104, 105])
      ç¬¬äºŒä¸ªæ ·æœ¬è¾“å…¥: tensor([102, 103, 104, 105])
      ç¬¬äºŒä¸ªæ ·æœ¬ç›®æ ‡: tensor([103, 104, 105, 106])
    æ­¥é•¿ä¸º 2 æ—¶çš„æ ·æœ¬æ•°: 4
      ç¬¬ä¸€ä¸ªæ ·æœ¬è¾“å…¥: tensor([101, 102, 103, 104])
      ç¬¬ä¸€ä¸ªæ ·æœ¬ç›®æ ‡: tensor([102, 103, 104, 105])
      ç¬¬äºŒä¸ªæ ·æœ¬è¾“å…¥: tensor([103, 104, 105, 106])
      ç¬¬äºŒä¸ªæ ·æœ¬ç›®æ ‡: tensor([104, 105, 106, 107])
    æ­¥é•¿ä¸º 3 æ—¶çš„æ ·æœ¬æ•°: 3
      ç¬¬ä¸€ä¸ªæ ·æœ¬è¾“å…¥: tensor([101, 102, 103, 104])
      ç¬¬ä¸€ä¸ªæ ·æœ¬ç›®æ ‡: tensor([102, 103, 104, 105])
      ç¬¬äºŒä¸ªæ ·æœ¬è¾“å…¥: tensor([104, 105, 106, 107])
      ç¬¬äºŒä¸ªæ ·æœ¬ç›®æ ‡: tensor([105, 106, 107, 108])
    

## 7.åˆ›å»ºè¯åµŒå…¥

è¯åµŒå…¥æ˜¯å°†æ ‡è®° ID è½¬æ¢ä¸ºè¿ç»­å‘é‡çš„è¿‡ç¨‹ï¼Œæ˜¯ LLM å¤„ç†æ–‡æœ¬çš„æœ€ç»ˆæ•°å€¼å½¢å¼ï¼ˆå¦‚å›¾ 2.15 æ‰€ç¤ºï¼‰ã€‚ç¥ç»ç½‘ç»œéœ€è¿ç»­å‘é‡è¿›è¡Œè¿ç®—ï¼Œå› æ­¤éœ€é€šè¿‡ â€œåµŒå…¥å±‚â€ å®ç°è¿™ä¸€è½¬æ¢ã€‚åµŒå…¥å±‚æœ¬è´¨æ˜¯ä¸€ä¸ªæƒé‡çŸ©é˜µï¼šè¡Œæ•°ç­‰äºè¯æ±‡è¡¨å¤§å°ï¼Œåˆ—æ•°ä¸ºåµŒå…¥ç»´åº¦ï¼ˆå¦‚ 3 ç»´ã€256 ç»´ï¼‰ï¼Œæ¯ä¸ªæ ‡è®° ID å¯¹åº”çŸ©é˜µä¸­çš„ä¸€è¡Œï¼Œå³è¯¥æ ‡è®°çš„åµŒå…¥å‘é‡ã€‚

ä¾‹å¦‚ï¼Œè¯æ±‡è¡¨å¤§å°ä¸º 6ã€åµŒå…¥ç»´åº¦ä¸º 3 çš„åµŒå…¥å±‚ï¼Œå…¶æƒé‡çŸ©é˜µä¸º 6Ã—3 çš„éšæœºåˆå§‹åŒ–çŸ©é˜µï¼ˆè®­ç»ƒä¸­ä¼šä¼˜åŒ–ï¼‰ã€‚å½“è¾“å…¥æ ‡è®° ID ä¸º [2, 3, 5, 1] æ—¶ï¼ŒåµŒå…¥å±‚ä¼šæå–çŸ©é˜µä¸­å¯¹åº”è¡Œï¼Œå¾—åˆ° 4Ã—3 çš„åµŒå…¥å‘é‡çŸ©é˜µã€‚

åœ¨ PyTorch ä¸­ï¼Œå¯é€šè¿‡ torch.nn.Embedding å®ç°åµŒå…¥å±‚ï¼Œå…¶æ ¸å¿ƒæ˜¯ â€œæŸ¥æ‰¾æ“ä½œâ€â€”â€” æ ¹æ® ID å¿«é€Ÿæ£€ç´¢å¯¹åº”åµŒå…¥å‘é‡ã€‚åµŒå…¥ç»´åº¦éœ€æƒè¡¡ï¼šæ›´é«˜ç»´åº¦ï¼ˆå¦‚ GPT-3 çš„ 12288 ç»´ï¼‰èƒ½æ•æ‰æ›´å¤šè¯­ä¹‰ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜ï¼›è¾ƒä½ç»´åº¦ï¼ˆå¦‚ 256 ç»´ï¼‰é€‚åˆå®éªŒã€‚åµŒå…¥å±‚çš„æƒé‡ä¼šé€šè¿‡è®­ç»ƒä¸æ–­ä¼˜åŒ–ï¼Œä½¿å‘é‡èƒ½æ›´å¥½åœ°è¡¨ç¤ºæ ‡è®°çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡


```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import List, Dict, Tuple, Optional, Set

class Vocabulary:
    """è¯æ±‡è¡¨ç®¡ç†ç±»"""
    
    def __init__(self, special_tokens: Optional[Dict[str, int]] = None):
        """
        åˆå§‹åŒ–è¯æ±‡è¡¨
        
        Args:
            special_tokens: ç‰¹æ®Šæ ‡è®°åŠå…¶IDçš„å­—å…¸
        """
        self.token_to_idx = special_tokens or {
            "<pad>": 0,
            "<unk>": 1,
            "<bos>": 2,
            "<eos>": 3
        }
        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}
        self.vocab_size = len(self.token_to_idx)
    
    def add_token(self, token: str) -> int:
        """
        æ·»åŠ æ ‡è®°åˆ°è¯æ±‡è¡¨
        
        Args:
            token: è¦æ·»åŠ çš„æ ‡è®°
            
        Returns:
            æ ‡è®°çš„ID
        """
        if token not in self.token_to_idx:
            self.token_to_idx[token] = self.vocab_size
            self.idx_to_token[self.vocab_size] = token
            self.vocab_size += 1
        return self.token_to_idx[token]
    
    def build_from_texts(self, texts: List[List[str]]) -> None:
        """
        ä»æ–‡æœ¬åˆ—è¡¨æ„å»ºè¯æ±‡è¡¨
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æœ¬æ˜¯æ ‡è®°åˆ—è¡¨
        """
        for text in texts:
            for token in text:
                self.add_token(token)
    
    def encode(self, tokens: List[str]) -> List[int]:
        """
        å°†æ ‡è®°åˆ—è¡¨ç¼–ç ä¸ºIDåˆ—è¡¨
        
        Args:
            tokens: æ ‡è®°åˆ—è¡¨
            
        Returns:
            IDåˆ—è¡¨
        """
        return [self.token_to_idx.get(token, self.token_to_idx["<unk>"]) for token in tokens]
    
    def decode(self, ids: List[int]) -> List[str]:
        """
        å°†IDåˆ—è¡¨è§£ç ä¸ºæ ‡è®°åˆ—è¡¨
        
        Args:
            ids: IDåˆ—è¡¨
            
        Returns:
            æ ‡è®°åˆ—è¡¨
        """
        return [self.idx_to_token.get(idx, "<unk>") for idx in ids]


class CBOWDataset(Dataset):
    """è¿ç»­è¯è¢‹æ¨¡å‹(CBOW)çš„æ•°æ®é›†"""
    
    def __init__(self, texts: List[List[str]], vocab: Vocabulary, context_size: int = 2):
        """
        åˆå§‹åŒ–CBOWæ•°æ®é›†
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æœ¬æ˜¯æ ‡è®°åˆ—è¡¨
            vocab: è¯æ±‡è¡¨
            context_size: ä¸Šä¸‹æ–‡å¤§å°(æ¯ä¾§çš„è¯æ•°)
        """
        self.context_size = context_size
        self.vocab = vocab
        self.data = []
        
        # æ„å»ºè®­ç»ƒæ ·æœ¬
        for text in texts:
            encoded_text = vocab.encode(text)
            for i in range(context_size, len(encoded_text) - context_size):
                context = []
                # æ”¶é›†å·¦å³ä¸Šä¸‹æ–‡
                for j in range(-context_size, context_size + 1):
                    if j != 0:  # è·³è¿‡ä¸­å¿ƒè¯
                        context.append(encoded_text[i + j])
                target = encoded_text[i]  # ä¸­å¿ƒè¯
                self.data.append((context, target))
    
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        Args:
            idx: æ ·æœ¬ç´¢å¼•
            
        Returns:
            å…ƒç»„(context, target)ï¼Œå…¶ä¸­contextæ˜¯ä¸Šä¸‹æ–‡è¯IDï¼Œtargetæ˜¯ç›®æ ‡è¯ID
        """
        context, target = self.data[idx]
        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)


class CBOW(nn.Module):
    """è¿ç»­è¯è¢‹æ¨¡å‹(CBOW)"""
    
    def __init__(self, vocab_size: int, embedding_dim: int):
        """
        åˆå§‹åŒ–CBOWæ¨¡å‹
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            embedding_dim: åµŒå…¥ç»´åº¦
        """
        super(CBOW, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)
        
    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            inputs: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º[batch_size, context_size*2]
            
        Returns:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º[batch_size, vocab_size]
        """
        # è·å–ä¸Šä¸‹æ–‡è¯çš„åµŒå…¥
        embeds = self.embeddings(inputs)
        # å¯¹ä¸Šä¸‹æ–‡åµŒå…¥å–å¹³å‡
        context_mean = torch.mean(embeds, dim=1)
        # é€šè¿‡çº¿æ€§å±‚é¢„æµ‹ä¸­å¿ƒè¯
        output = self.linear(context_mean)
        return output


def train_cbow_model(texts: List[List[str]], embedding_dim: int = 100, 
                     context_size: int = 2, epochs: int = 10, 
                     batch_size: int = 32, lr: float = 0.01) -> nn.Embedding:
    """
    è®­ç»ƒCBOWæ¨¡å‹å¹¶è¿”å›è¯åµŒå…¥
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æœ¬æ˜¯æ ‡è®°åˆ—è¡¨
        embedding_dim: åµŒå…¥ç»´åº¦
        context_size: ä¸Šä¸‹æ–‡å¤§å°
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        lr: å­¦ä¹ ç‡
        
    Returns:
        è®­ç»ƒå¥½çš„è¯åµŒå…¥å±‚
    """
    # æ„å»ºè¯æ±‡è¡¨
    vocab = Vocabulary()
    vocab.build_from_texts(texts)
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = CBOWDataset(texts, vocab, context_size)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = CBOW(vocab.vocab_size, embedding_dim)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    # è®­ç»ƒæ¨¡å‹
    for epoch in range(epochs):
        total_loss = 0
        for context, target in dataloader:
            # å‰å‘ä¼ æ’­
            output = model(context)
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}")
    
    # è¿”å›è®­ç»ƒå¥½çš„è¯åµŒå…¥å±‚
    return model.embeddings


# æµ‹è¯•ä»£ç 
def test_word_embedding():
    """æµ‹è¯•è¯åµŒå…¥åŠŸèƒ½"""
    print("===== æµ‹è¯•è¯åµŒå…¥ =====")
    
    # ç¤ºä¾‹æ–‡æœ¬
    texts = [
        ["I", "like", "to", "play", "football"],
        ["Football", "is", "a", "popular", "sport"],
        ["I", "enjoy", "watching", "football", "matches"],
        ["Do", "you", "play", "any", "sports"],
        ["Sports", "are", "good", "for", "health"]
    ]
    
    # è®­ç»ƒCBOWæ¨¡å‹è·å–è¯åµŒå…¥
    embedding_dim = 10
    context_size = 2
    embeddings = train_cbow_model(
        texts=texts,
        embedding_dim=embedding_dim,
        context_size=context_size,
        epochs=50,
        batch_size=4,
        lr=0.01
    )
    
    # è·å–è¯æ±‡è¡¨
    vocab = Vocabulary()
    vocab.build_from_texts(texts)
    
    # æµ‹è¯•è¯åµŒå…¥æŸ¥æ‰¾
    test_words = ["I", "football", "sports", "unknown"]
    print("\nè¯åµŒå…¥ç¤ºä¾‹:")
    for word in test_words:
        word_id = vocab.encode([word])[0]
        word_vector = embeddings(torch.tensor(word_id, dtype=torch.long)).detach().numpy()
        print(f"{word}: {word_vector[:5]}... (shape: {word_vector.shape})")
    
    # è®¡ç®—è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦
    print("\nè¯ç›¸ä¼¼åº¦åˆ†æ:")
    target_words = ["football", "sports", "play"]
    for target in target_words:
        target_id = vocab.encode([target])[0]
        target_vector = embeddings(torch.tensor(target_id, dtype=torch.long))
        
        print(f"\nä¸ '{target}' æœ€ç›¸ä¼¼çš„è¯:")
        similarities = []
        for word, idx in vocab.token_to_idx.items():
            if word in ["<pad>", "<unk>", "<bos>", "<eos>"]:
                continue
            
            word_vector = embeddings(torch.tensor(idx, dtype=torch.long))
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            sim = torch.cosine_similarity(target_vector.unsqueeze(0), 
                                         word_vector.unsqueeze(0)).item()
            similarities.append((word, sim))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # æ‰“å°å‰3ä¸ªç›¸ä¼¼è¯
        for word, sim in similarities[:3]:
            print(f"  {word}: {sim:.4f}")

if __name__ == "__main__":
    test_word_embedding()    
```

    ===== æµ‹è¯•è¯åµŒå…¥ =====
    Epoch 1/50, Loss: 3.1725
    Epoch 2/50, Loss: 3.2624
    Epoch 3/50, Loss: 3.1252
    Epoch 4/50, Loss: 2.7059
    Epoch 5/50, Loss: 2.4706
    Epoch 6/50, Loss: 2.3471
    Epoch 7/50, Loss: 2.5184
    Epoch 8/50, Loss: 2.4183
    Epoch 9/50, Loss: 2.3172
    Epoch 10/50, Loss: 2.2042
    Epoch 11/50, Loss: 2.2335
    Epoch 12/50, Loss: 1.9774
    Epoch 13/50, Loss: 1.8652
    Epoch 14/50, Loss: 1.7527
    Epoch 15/50, Loss: 1.8568
    Epoch 16/50, Loss: 1.6976
    Epoch 17/50, Loss: 1.4242
    Epoch 18/50, Loss: 1.2158
    Epoch 19/50, Loss: 1.1381
    Epoch 20/50, Loss: 1.0590
    Epoch 21/50, Loss: 1.2977
    Epoch 22/50, Loss: 1.3026
    Epoch 23/50, Loss: 0.9900
    Epoch 24/50, Loss: 0.7884
    Epoch 25/50, Loss: 1.0041
    Epoch 26/50, Loss: 0.8770
    Epoch 27/50, Loss: 0.8645
    Epoch 28/50, Loss: 0.7021
    Epoch 29/50, Loss: 0.8588
    Epoch 30/50, Loss: 0.6019
    Epoch 31/50, Loss: 0.4728
    Epoch 32/50, Loss: 0.5092
    Epoch 33/50, Loss: 0.4671
    Epoch 34/50, Loss: 0.4910
    Epoch 35/50, Loss: 0.5640
    Epoch 36/50, Loss: 0.5198
    Epoch 37/50, Loss: 0.4731
    Epoch 38/50, Loss: 0.4264
    Epoch 39/50, Loss: 0.3248
    Epoch 40/50, Loss: 0.2492
    Epoch 41/50, Loss: 0.2738
    Epoch 42/50, Loss: 0.3748
    Epoch 43/50, Loss: 0.2021
    Epoch 44/50, Loss: 0.2448
    Epoch 45/50, Loss: 0.1978
    Epoch 46/50, Loss: 0.2853
    Epoch 47/50, Loss: 0.1764
    Epoch 48/50, Loss: 0.1442
    Epoch 49/50, Loss: 0.1716
    Epoch 50/50, Loss: 0.2133
    
    è¯åµŒå…¥ç¤ºä¾‹:
    I: [-0.0588957  -0.14068426 -0.7404043  -1.8865429  -2.6835012 ]... (shape: (10,))
    football: [ 0.637025    0.14052066 -0.848007    0.2889565  -0.2740498 ]... (shape: (10,))
    sports: [-1.045044   -0.70745003 -2.0171206   1.0361644   0.60308105]... (shape: (10,))
    unknown: [-0.14042336  0.71746343  0.11502329 -0.5219049   0.10613117]... (shape: (10,))
    
    è¯ç›¸ä¼¼åº¦åˆ†æ:
    
    ä¸ 'football' æœ€ç›¸ä¼¼çš„è¯:
      football: 1.0000
      is: 0.7577
      health: 0.6264
    
    ä¸ 'sports' æœ€ç›¸ä¼¼çš„è¯:
      sports: 1.0000
      health: 0.5747
      is: 0.5725
    
    ä¸ 'play' æœ€ç›¸ä¼¼çš„è¯:
      play: 1.0000
      popular: 0.6973
      is: 0.6605
    

## 8.ç¼–ç è¯ä½ç½®

LLM çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æœ¬èº«ä¸æ„ŸçŸ¥è¯çš„ä½ç½®ï¼Œå› æ­¤éœ€åŠ å…¥ â€œä½ç½®åµŒå…¥â€ ä»¥ä¼ è¾¾è¯åœ¨åºåˆ—ä¸­çš„é¡ºåºä¿¡æ¯ï¼ˆå¦‚å›¾ 2.17 æ‰€ç¤ºï¼‰ã€‚ä½ç½®åµŒå…¥æœ‰ä¸¤ç§ç±»å‹ï¼šç»å¯¹ä½ç½®åµŒå…¥ï¼ˆä¸å…·ä½“ä½ç½®ç»‘å®šï¼Œå¦‚ç¬¬ 1 ä¸ªè¯ç”¨ç‰¹å®šå‘é‡ï¼Œç¬¬ 2 ä¸ªè¯ç”¨å¦ä¸€å‘é‡ï¼‰å’Œç›¸å¯¹ä½ç½®åµŒå…¥ï¼ˆå…³æ³¨è¯ä¹‹é—´çš„è·ç¦»ï¼Œå¦‚ â€œç›¸è· 2 ä¸ªä½ç½®â€ï¼‰ã€‚

GPT æ¨¡å‹é‡‡ç”¨ç»å¯¹ä½ç½®åµŒå…¥ï¼Œå…¶å®ç°æ–¹å¼æ˜¯ï¼šåˆ›å»ºå¦ä¸€ä¸ªåµŒå…¥å±‚ï¼Œè¾“å…¥ä¸º 0 è‡³ä¸Šä¸‹æ–‡é•¿åº¦ - 1 çš„ä½ç½®ç´¢å¼•ï¼ˆå¦‚çª—å£å¤§å°ä¸º 4 æ—¶ï¼Œè¾“å…¥ä¸º [0, 1, 2, 3]ï¼‰ï¼Œè¾“å‡ºä¸è¯åµŒå…¥ç»´åº¦ç›¸åŒçš„ä½ç½®å‘é‡ã€‚æœ€ç»ˆè¾“å…¥åµŒå…¥æ˜¯è¯åµŒå…¥ä¸ä½ç½®åµŒå…¥çš„æ€»å’Œï¼ˆå¦‚å›¾ 2.19 æ‰€ç¤ºï¼‰ï¼Œä¾‹å¦‚æŸè¯çš„åµŒå…¥å‘é‡ä¸º [1.2, -0.2, -0.1]ï¼Œå…¶ä½ç½®åµŒå…¥ä¸º [0.5, 0.3, 0.1]ï¼Œåˆ™æœ€ç»ˆè¾“å…¥ä¸º [1.7, 0.1, 0.0]ã€‚

ä½ç½®åµŒå…¥çš„ç»´åº¦ä¸è¯åµŒå…¥ä¸€è‡´ï¼Œç¡®ä¿ä¸¤è€…å¯ç›´æ¥ç›¸åŠ ï¼Œä¸”ä¼šéšæ¨¡å‹è®­ç»ƒä¼˜åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å­¦ä¹ åˆ°è¯åºå¯¹è¯­ä¹‰çš„å½±å“ï¼ˆå¦‚ â€œæˆ‘çˆ±ä½ â€ ä¸ â€œä½ çˆ±æˆ‘â€ çš„åŒºåˆ«ï¼‰ã€‚


```python
import torch
import tiktoken
# 1åˆå§‹åŒ–åˆ†è¯å™¨ï¼ˆä½¿ç”¨GPT-2çš„BPEåˆ†è¯å™¨ï¼‰
tokenizer = tiktoken.get_encoding("gpt2")
# å®šä¹‰åµŒå…¥å‚æ•°
vocab_size = 50257  # GPT-2çš„è¯æ±‡è¡¨å¤§å°
output_dim = 256    # åµŒå…¥ç»´åº¦ï¼ˆç¤ºä¾‹ç”¨256ï¼Œå®é™…GPT-3ä¸º12288ï¼‰
context_length = 4  # ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå³è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼‰

# åˆ›å»ºè¯åµŒå…¥å±‚å’Œä½ç½®åµŒå…¥å±‚
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)

# ç”Ÿæˆä½ç½®ç´¢å¼•ï¼ˆ0åˆ°context_length-1ï¼‰
pos_indices = torch.arange(context_length)  # å½¢çŠ¶: [4]
pos_embeddings = pos_embedding_layer(pos_indices)  # å½¢çŠ¶: [4, 256]

# æµ‹è¯•ï¼šå°†è¯IDè½¬æ¢ä¸ºåµŒå…¥å¹¶æ·»åŠ ä½ç½®åµŒå…¥
def test_position_embedding():
    # ç¤ºä¾‹è¾“å…¥è¯ID
    input_ids = torch.tensor([
        [40, 367, 2885, 1464],    # ç¬¬ä¸€å¥çš„è¯ID
        [1807, 3619, 402, 271],   # ç¬¬äºŒå¥çš„è¯ID
        [10899, 2138, 257, 7026]  # ç¬¬ä¸‰å¥çš„è¯ID
    ])
    batch_size, seq_len = input_ids.shape
    print(f"è¾“å…¥è¯IDå½¢çŠ¶: {input_ids.shape}")  # åº”è¾“å‡º: torch.Size([3, 4])

    # ç”Ÿæˆè¯åµŒå…¥
    token_embeddings = token_embedding_layer(input_ids)
    print(f"è¯åµŒå…¥å½¢çŠ¶: {token_embeddings.shape}")  # åº”è¾“å‡º: torch.Size([3, 4, 256])

    # æ·»åŠ ä½ç½®åµŒå…¥
    input_embeddings = token_embeddings + pos_embeddings
    print(f"æ·»åŠ ä½ç½®åµŒå…¥åçš„å½¢çŠ¶: {input_embeddings.shape}")  # åº”è¾“å‡º: torch.Size([3, 4, 256])

    # éªŒè¯ä½ç½®åµŒå…¥çš„å”¯ä¸€æ€§
    print("\nä½ç½®åµŒå…¥å‘é‡ï¼ˆå‰3ä¸ªä½ç½®çš„å‰5ç»´ï¼‰:")
    for i in range(3):
        print(f"ä½ç½® {i}: {pos_embeddings[i, :5]}")

# æ‰§è¡Œæµ‹è¯•
if __name__ == "__main__":
    test_position_embedding()
```

    è¾“å…¥è¯IDå½¢çŠ¶: torch.Size([3, 4])
    è¯åµŒå…¥å½¢çŠ¶: torch.Size([3, 4, 256])
    æ·»åŠ ä½ç½®åµŒå…¥åçš„å½¢çŠ¶: torch.Size([3, 4, 256])
    
    ä½ç½®åµŒå…¥å‘é‡ï¼ˆå‰3ä¸ªä½ç½®çš„å‰5ç»´ï¼‰:
    ä½ç½® 0: tensor([-0.3552, -0.5629, -1.4778,  0.7029, -0.0278], grad_fn=<SliceBackward0>)
    ä½ç½® 1: tensor([-0.7520,  0.3258,  0.5109, -1.2897,  0.2495], grad_fn=<SliceBackward0>)
    ä½ç½® 2: tensor([-0.6930,  0.9321, -0.9753,  0.5288,  0.8013], grad_fn=<SliceBackward0>)
    

## è´¡çŒ®è€…ä¸»é¡µ


|è´¡çŒ®è€…|å­¦æ ¡  | ç ”ç©¶æ–¹å‘           |   GitHubä¸»é¡µ |
|-----------------|------------------------|-----------------------|------------|
| è”¡é‹†æ· | ç¦å·å¤§å­¦  |    Computer Visionï¼ˆCVï¼‰ï¼ŒNatural Language Processingï¼ˆNLPï¼‰      |https://github.com/xinala-781|
